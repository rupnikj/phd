%--------------------------------------------------------------------------------------------------
%
\chapter{Cross-Lingual Document Similarity}\label{chap:crosslingual}
%--------------------------------------------------------------------------------------------------

Document similarity is an important component in techniques from text mining and natural language processing.
Many techniques use the similarity as a black box, e.g., a kernel in Support Vector Machines.
Comparison of documents (or other types of text snippets) in a monolingual setting is a
well-studied problem in the field of information retrieval~\cite{Salton88term-weightingapproaches}.
We first formally introduce the problem followed by a description of our novel approach.

\section{Problem Definition}\label{chap:crosslingual:problem}
We will first describe how documents are represented as vectors and how to compare documents in
a mono-lingual setting. We then define a way to measure cross-lingual similarity which is natural
for the models we consider.

\bolded{Document Representation.}
The standard vector space model~\cite{Salton88term-weightingapproaches} represents documents as
vectors, where each term corresponds to a word or a phrase in a fixed vocabulary. Formally,
document $d$ is represented by a vector $x \in \RR^n$, where $n$ corresponds to the size
of the vocabulary, and vector elements $x_k$ correspond to the number of times term $k$
occurred in the document, also called \emph{term frequency} or $TF_k(d)$.

For our document representation we applied a term re-weighting scheme that adjusts
for the fact that some words occur more frequently in general. A term weight should correspond to the importance of the term for the given corpus. The common weighting scheme is called \emph{Term Frequency Inverse Document Frequency} ($TFIDF$) weighting. An \emph{Inverse Document Frequency} ($IDF$) weight for the dictionary term $k$ is defined as $\log\left( \frac{N}{DF_k} \right)$, where $DF_k$ is the number of documents in the corpus which contain term $k$ and $N$ is the total number of documents in the corpus. In the other part of our system, we computed TFIDF vectors on streams
of news articles in multiple languages. There the IDF scores for each language changed
dynamically - for each new document we computed the IDF of all news articles within
a ten day window.

The $TFIDF$ weighted vector space model document representation corresponds
to a map $\phi : \text{text} \rightarrow \RR^n$ defined by:
$$\phi(d)_k = {TF}_k(d) \log\left( \frac{N}{{DF}_k}\right).$$

\bolded{Monolingual similarity.}
A common way of computing similarity between documents is \emph{cosine similarity},
$$\textnormal{sim}(d_1, d_2) = \frac{\langle \phi(d_1), \phi(d_2)\rangle}{\|\phi(d_1)\| \|\phi(d_2)\|},$$
where $\langle \cdot,\cdot \rangle$ and $\|\cdot\|$ are standard inner product and
Euclidean norm respectively. When dealing with two or more languages, one could ignore the language information
and build a vector space using the union of tokens over the languages. A cosine similarity
function in such a space can be useful to some extent, for example ``Internet'' or ``Bowie''
may appear both in Spanish and English texts and the presence of such terms in both an
English and a Spanish document would contribute to their similarity. In general however,
large parts of vocabularies may not intersect. This means that given a language pair,
many words in both languages cannot contribute to the similarity score. Such cases
can make the similarity function very insensitive to the data.

\bolded{Cross-Lingual Similarity.}
Processing a multilingual dataset results in several vector spaces with varying dimensionality,
one for each language. The dimensionality of the vector space corresponding to the $i$-th
language is denoted by $n_i$ and the vector space model mapping is denoted by
$\phi_i : \text{text} \rightarrow \RR^{n_i}$.
The similarity between documents in language $i$ and language $j$ is defined as a bilinear
operator represented as a matrix $S_{i,j} \in \RR^{n_i \times n_j}$:
$$\textnormal{sim}_{i,j}(d_1, d_2) = \frac{ \langle \phi_i (d_1), S_{i,j} \phi_j (d_2) \rangle }{\|\phi_i(d_1)\| \|\phi_j(d_2)\|},$$
where $d_1$ and $d_2$ are documents written in the $i$-th and $j$-th language respectively.
If the maximal singular value of $S_{i,j}$ is bounded by $1$, then the similarity scores
will lie on the interval $[-1, 1]$. We will provide an overview of the models in
Section~\ref{chap:crosslingual:models}, present related work in Section~\ref{chap:crosslingual:related}
and then introduce additional notation in Section~\ref{chap:crosslingual:notation}.
Starting with Section~\ref{chap:crosslingual:kmeans} and ending with Section~\ref{chap:crosslingual:hublang} we will
describe some approaches to compute $S_{i,j}$ given training data.

\section{Cross-Lingual Models}\label{chap:crosslingual:models}
In this chapter we will describe several approaches to the problem of computing the
multilingual similarities introduced in Section~\ref{chap:crosslingual:problem}. We present four approaches:
a simple approach based on $k$-means clustering in Section~\ref{chap:crosslingual:kmeans}, a standard approach
based on singular value decomposition in Section~\ref{chap:crosslingual:LSI}, a related
approach called Canonical Correlation Analysis (CCA) in Section~\ref{chap:crosslingual:CCA} and finally a
new method, which is an extension of CCA to more than two languages in Section~\ref{chap:crosslingual:hublang}.

CCA can be used to find correlated patterns for a pair of languages, whereas the extended method
optimizes a Sum of Squared Correlations (SSCOR) between several language pairs, which was introduced
in~\cite{Kettenring}. The SSCOR problem is difficult to solve in our setting (hundreds of thousands
of features, hundreds of thousands of examples). To tackle this, we propose a method which consists
of two ingredients. The first one is based on an observation that certain datasets (such as Wikipedia)
are biased towards one language (English for Wikipedia), which can be exploited to reformulate a
difficult optimization problem as an eigenvector problem. The second ingredient is dimensionality
reduction using CL-LSI, which makes the eigenvector problem computationally and numerically tractable.

We concentrate on approaches that are based on linear maps rather than alternatives, such as machine
translation and probabilistic models, as discussed in the section on related work. We will start
by introducing some notation.

The thesis so far focused on the SUMCOR problem formulation, where two extensions were proposed in Chapter~\ref{chap:extensions} while
Chapter~\ref{chap:relaxations} presented results on the problem complexity and global optimality bounds. In contrast, this chapter focuses
on the SSCOR formulation. This is due to the fact that under an additional assumption (data is biased towards one view),
the problem simplifies significantly (we could not derive a similar result for SUMCOR). At the end of the chapter we will comment further
on the SUMCOR formulation in light of the additional assumption which made SSCOR computationally tractable.

\section{Related Work}\label{chap:crosslingual:related}
In this section, we describe previous work described  in the literature. We have grouped the approaches to cross-lingual similarity computation as those that are based on: translation and dictionaries, probabilistic topic models, matrix factorization, monolingual models and neural network word embeddings. We also present some related work that also uses the Wikipedia as a language resource.

\textbf{Translation and dictionary based}. The most obvious way to compare documents written in different languages is to use machine translation and perform monolingual similarity, see~\cite{multilingualBook}~and~\cite{plagiarism} for several variations of translation based approaches. One can use free tools such as Moses~\cite{moses} or translation services, such as Google Translate\footnote{https://translate.google.com/}. There are two issues with such approaches: they solve a harder problem than needs to be solved and they are less robust to training resource quality - large sets of translated sentences are typically needed. Training Moses for languages with scarce linguistic resources is thus problematic. The issue with using online services such as Google Translate is that the APIs are limited and not free. The operation efficiency and cost requirements make translation-based approaches less suited for our system. Closely related are works Cross-Lingual Vector Space Model (CL-VSM)~\cite{plagiarism} and the approach presented in~\cite{pouliquen2008story} which both compare documents by using dictionaries, which in both cases are EuroVoc dictionaries~\cite{eurovoc}. The generality of such approaches is limited by the quality of available linguistic resources, which may be scarce or non-existent for certain language pairs.

\textbf{Probabilistic topic models}. There exist many variants to modelling documents in a language independent way by using probabilistic graphical models. The models include:  Joint Probabilistic Latent Semantic Analysis (JPLSA)~\cite{platt2010translingual}, Coupled Probabilistic LSA (CPLSA)~\cite{platt2010translingual}, Probabilistic Cross-Lingual LSA (PCLLSA)~\cite{PCL_LSA} and Polylingual Topic Models (PLTM)~\cite{polyLDA} which is a Bayesian version of PCLLSA. The methods (except for CPLSA) describe the multilingual document collections as samples from generative probabilistic models, with variations on the assumptions on the model structure. The topics represent latent variables that are used to generate observed variables (words), a process specific to each language. The parameter estimation is posed as an inference problem which is typically intractable and one usually solves it using approximate techniques. Most variants of solutions are based on Gibbs sampling or Variational Inference, which are nontrivial to implement and may require an experienced practitioner to be applied. Furthermore, representing a new document as a mixture of topics is another potentially hard inference problem which must be solved.

\textbf{Matrix factorization}. Several matrix factorization based approaches exist in the literature. The models include: Non-negative matrix factorization based~\cite{nonnegfactor_lsi}, Cross-Lingual Latent Semantic Indexing CL-LSI~\cite{cl_lsi}~and~\cite{multilingualBook}, Canonical Correlation Analysis (CCA)~\cite{Hotelling}, Oriented Principal Component Analysis (OPCA)~\cite{platt2010translingual}. The quadratic time and space dependency of the OPCA method makes it impractical for large scale purposes. In addition, OPCA forces the vocabulary sizes for all languages to be the same, which is less intuitive. For our setting, the method in ~\cite{nonnegfactor_lsi} has a prohibitively high computational cost when building models (it uses dense matrices whose dimensions are a product of the training set size and the vocabulary size). Our proposed approach combines CCA and CL-LSI. Another closely related method is Cross-Lingual Explicit Semantic Analysis (CL-ESA)~\cite{ESA}, which uses Wikipedia (as do we in the current work) to compare documents. It can be interpreted as using the sample covariance matrix between features of two languages to define the dot product which is used to compute similarities.
The authors of CL-ESA compare it to CL-LSI and find that CL-LSI can outperform CL-ESA in an information retrieval, but is costlier to optimize over a large corpus (CL-ESA requires no training). We find that the scalability argument does not apply in our case: based on advances in numerical linear algebra we can solve large CL-LSI problems (millions of documents as opposed to the 10,000 document limit reported in~\cite{ESA}\cite{sorg2012exploiting}). In addition, CL-ESA is less suited for computing similarities between two large monolingual streams. For example, each day we have to compute similarities between 500,000 English and 500,000 German news articles. Comparing each German news article with 500,000 English news articles is either prohibitively slow (involves projecting all English articles on Wikipedia) or consumes too much memory (involves storing the projected English articles, which for a Wikipedia of size 1,000,000 is a 500,000 by 1,000,000 non-sparse matrix). An interesting combination of
dictionaries, CCA and LSI is presented in~\cite{faruqui-dyer:2014:EACL}, where the authors show how to
incorporate multilingual evidence into independently built (monolingual) vector spaces. In~\cite{pozniak2016optimization} the authors investigated how to optimally select an LSI training dataset for specific classification tasks and thus built domain specific common document representations, which is interesting but less relevant to our problem setting.

\textbf{Monolingual}. Related work also includes monolingual approaches that treat document written in different languages in a monolingual fashion. The intuition is that named entities (for example, ``Bowie'') and cognate words (for example, ``tsunami'') are written in the same or similar fashion in many languages. For example, the Cross-Language Character n-Gram Model (CL-CNG)~\cite{plagiarism} represents documents as bags of character $n$-grams. Another approach is to use language dependent keyword lists based on cognate words ~\cite{pouliquen2008story}. These approaches may be suitable for comparing documents written in languages that share a writing system, which does not apply to the case of global news tracking.

\textbf{Word embeddings and neural networks}.
Many approaches in the recent literature focus on neural network models which construct hierarchical (distributed) representations of data. See~\cite{Hermann:2014:ACLphil},\cite{DBLP:conf/acl/VulicM15},\cite{lu2015deep} and~\cite{bicompare:16} for an empirical comparison of several architectures over a variety of tasks. The approaches typically involve many tuning parameters (learning rates, batch sizes, neural network architectures) and the training is tends to be computationally intensive.

\textbf{Wikipedia based}. Finally, there are some works that use Wikipedia to build models, but not necessarily focused at cross-lingual similarity learning. For example~\cite{SPITKOVSKY12.266} use Wikipedia in order to build a cross-lingual dictionary that maps phrases in other languages to English wikipedia concepts. Wikipedia was also used to build an entity matching model~\cite{rinser2013cross}, with the aim of aligning Wikipedia infoboxes. In~ \cite{barron2014comparison} the authors investigated several cross-lingual measures between web documents (based on link analysis, as well as the distribution of cognate words, character n-grams etc.) and found it very useful for under-resourced languages. Another paper~\cite{sogaard2015inverted} used Wikipedia concepts as an inverted index to build cross-lingual similarities. The approach represented words as distributions over Wikipedia concepts, which is closely related to ~\cite{ESA}.


\section{Notation}\label{chap:crosslingual:notation}

The cross-lingual similarity models presented in this paper are based on comparable corpora.
A \emph{comparable corpus} is a collection of documents in multiple languages, with alignment
between documents that are of the same topic, or even a rough translation of each other.
Wikipedia is an example of a comparable corpus, where a specific entry can be described in multiple
languages (e.g., ``Berlin" is currently described in 222 languages). News articles represent
another example, where the same event can be described by newspapers in several languages.

More formally, a \emph{multilingual document} $d = (u_1,\ldots u_m)$ is a tuple of $m$ documents
on the same topic (comparable), where $u_i$ is the document written in language $i$. Note that
an individual document $u_i$ can be an empty document (missing resource) and each $d$ must contain
at least \textbf{two nonempty documents}. This means that in our analysis we discard strictly
monolingual documents for which no cross-lingual information is available. A comparable corpus
$D = {d_1, \ldots, d_s}$ is a collection of $s$ multilingual documents. By using the vector space
model, we can represent $D$ as a set of $m$ multiview aligned matrices:
$X_1,\ldots,X_m$, where $X_i \in \RR^{n_i \times s}$
is the matrix corresponding to the language $i$ and $n_i$ is the vocabulary size of language $i$.
Furthermore, let $X_i(:,\ell)$ denote the $\ell$-th column of matrix $X_i$ and the matrices respect
the document alignment - the vector $X_i(:,\ell)$ corresponds to the TFIDF vector of the $i$-th component
of multilingual document $d_\ell$. We use $N$ to denote the total row dimension of $X$, i.e.,
$N:= \sum_{i=1}^m n_i$. See Figure~\ref{fig:stacked_matrices} for an illustration of the introduced notation.
The dimensions of each view form a block structure $b := (b_1, \ldots, b_m)$ and we will use the parenthesis
notation $x^{(i)}$ to denote the $i$-th block according to that block structure.

\begin{figure}[tbp]
\centering
\includegraphics[width=9cm]{figures/stacked_matrices1-crop.pdf}
\caption[Multilingual corpus matrices]{Multilingual corpora and their matrix representations using the vector space model.}
\label{fig:stacked_matrices}
\end{figure}

We will now describe four models to cross-lingual similarity computation, where the first three are based on
methods introduced in Chapter~\ref{chap:background} and the fourth one represents an original contribution.
\section{$k$-means}\label{chap:crosslingual:kmeans}

We have introduced the $k$-means algorithm in Section~\ref{chap:background:kmeans} and we now discuss
how to apply it to build a cross-lingual similarity function. In a nutshell, we apply the standard
$k$-means procedure to the stacked multiview matrix and interpret the centroids as an aligned basis, which
is used to build the CL-similarity model; see Figure~\ref{fig:kmeans} for an illustration of the approach.

\begin{figure}[tbp]
\centering
\includegraphics[width=10cm]{figures/kmeans.pdf}
\caption{$k$-means algorithm and coordinate change.}
\label{fig:kmeans}
\end{figure}

In order to apply the algorithm, we first merge all the term-document matrices into a single matrix
$X$ by stacking the individual term-document matrices (as seen in Figure~\ref{fig:stacked_matrices}):
$$X := \begin{bmatrix}X_1^T ,X_2^T, \cdots, X_m^T \end{bmatrix}^T,$$
such that the columns respect the alignment of the documents (here MATLAB notation for concatenating
matrices is used). Therefore, each document is represented by a long vector indexed by the terms in all languages.

We then run the $k$-means algorithm and obtain a centroid matrix $C \in \RR^{N \times k}$,
where the $k$ columns represent centroid vectors. The centroid matrix can be split vertically into $m$
blocks: $$C = [C^{(1)T} \cdots C^{(m^T)}]^T,$$ according to the number of dimensions of each language,
i.e., $C^{(i)} \in \RR^{n_i \times k}$.

To reiterate, the matrices $C^{(i)}$ are computed using a multilingual corpus
matrix $X$ (based on Wikipedia for example).

To compute  cross-lingual document similarities on new documents, we interpret each matrix $C^{(i)}$
as a vector space basis which can be used to map points in $\RR^{n_i}$ into a $k$-dimensional space. Given
a new vector $x \in \RR^{n_i}$ we can express its coordinates as:
$$(C^{(i)T} C^{(i)})^{-1} C^{(i)T} x_i.$$

The resulting matrix for similarity computation between language $i$ and language $j$
is defined up to a scaling factor as:
$$C^{(i)}(C^{(i)T} C^{(i)})^{-1} (C^{(j)T} C^{(j)})^{-1} C^{(j)T}.$$

The matrix is a result of mapping documents in a language independent space using
pseudo-inverses of the centroid matrices $P_i = (C^{(i)T} C^{(i)})^{-1} C^{(i)T}$ and then
comparing them using the standard inner product, which results in the matrix
$P_i^T P_j$. For the sake of presentation, we assumed that the centroid vectors
are linearly independent. (An independent subspace could be obtained using an
additional Gram-Schmidt step~\cite{golub} on the matrix $C$, if this was not the case.)

\section{Cross-Lingual Latent Semantic Indexing}\label{chap:crosslingual:LSI}

Section~\ref{chap:background:svd} introduced two closely related approaches
to pattern analysis based on spectral decompositions. This section summarizes
how these approaches apply to cross-lingual document analysis. Truncated Singular
value Decomposition (TSVD) applied to monolingual document analysis was introduced
in~\cite{lsi} where it is referred to as Latent Semantic Indexing (LSI). An extension
to cross-lingual document analysis was proposed in~\cite{cl_lsi} and is referred to as
Cross-Lingual Latent Semantic Indexing (CL-LSI). Now follows a description of a
variation of CL-LSI, relevant to the thesis.

The method is based on computing a truncated singular value decomposition of
multiview stacked matrix $X \approx U S V^T$. See Figure~\ref{fig:lsi} for the decomposition.
Representing documents in ``topic'' coordinates is done in the same way as in the $k$-means case
(see Figure~\ref{fig:kmeans}), we will describe how to compute the coordinate change functions.

\begin{figure}[tbp]
\centering
\includegraphics[width=10cm]{figures/lsi.pdf}
\caption{LSI multilingual corpus matrix decomposition.}
\label{fig:lsi}
\end{figure}

The cross-lingual similarity functions are based on a rank-$k$ truncated SVD:
$X \approx U \Sigma V^T,$ where $U \in \RR^{N \times k}$ are basis vectors of
interest and $\Sigma \in \RR^{k \times k}$ is a truncated diagonal matrix of singular
eigenvalues. An aligned basis is obtained by first splitting $U$ vertically according
to the number of dimensions of each language: $U = [U^{(i)T} \cdots U^{(m)T}]^T$.
The standard CL-LSI approach is to use matrices $U_i$ directly as maps to the common
coordinate space - the cross-lingual similarity function between two documents $x_i$ and
$x_j$ is given by $x_i^T U^{(i)} S^{-1} S^{-1} U^{(j)T} x_j$. We note that in general the blocks
are not orthogonal and the norms of their columns may vary. Our modification to the
standard approach is to treat $U^{(i)}$ as aligned bases, and use their pseudo-inverses
to construct projection maps (exactly $k$-means centroids were treated). Assuming
that the blocks $X^{(i)}$ have full rank (and thus so are $U^{(i)}$),
the pseudo-inverses can be expressed as $P_i := (U^{(i)T} U^{(i)})^{-1} U^{(i)T}$.
The matrices $P_i$ are used to change the basis from the standard basis in $\RR^{n_i}$ to the
basis spanned by the columns of $U_i$.

\bolded{Implementation note.}
Since the matrix $X$ can be large we could use an iterative method like the Lanczos
algorithm with re-orthogonalization~\cite{golub} to find the left singular vectors
(columns of $U$) corresponding to the largest singular values. In our experiments
the Lanczos method converged slowly indicating problems with singular value separation.
Moreover, the Lanczos method is hard to parallelize. Instead, we use a randomized version
of the SVD~\cite{tropp} that can be viewed as a block Lanczos method. That enables us
to use parallelization and speeds up the computation considerably.

To compute the matrices $P_i$ we used the QR algorithm~\cite{golub} to factorize
$U^{(i)}$ as $U^{(i)} = Q_i R_i$, where $Q_i^TQ_i = I$ and $R_i$ is a triangular matrix.
$P_i$ is then obtained by solving $R_i P_i = Q_i$.

\section{Bi-Lingual Document Analysis CCA}\label{chap:crosslingual:CCA}
CCA can be applied to bi-lingual document analysis given two languages. Let $X_1 \in \RR^{p \times \ell}$
and $X_2 \in ^{q \times \ell}$ denote the two multiview aligned matrices based on a bi-lingual aligned
document collection vectorized using two vector space models.
If $W^{(1)}\in \RR^{p\times k}$ and $W^{(2)} \in \RR^{q\times k}$ are solutions comprising of $k$ canonical
correlation vector pairs (corresponding to columns of $W^{(1)}$ and $W^{(2)}$)
of the generalized eigenvalue problem in Equation~\ref{eq:background:cca:eigen}, then
the bi-lingual similarity function that we use is expressed as a cosine similarity
after applying the canonical maps:
$$\textnormal{sim}(x_1, x_2) \propto \frac{x_1^T W^{(1)}W^{(2)^T}x_2}{\norm{W^{(1)T}x_1} \norm{W^{(2)T}x_2}} .$$
The intuition behind using the matrices as projectors (even though their columns are not an
orthonormal basis) is that the CCA is equivalent to minimizing the expected
distance between $W^{(1)T}\mathcal{X}^{(1)}$ and $W^{(2)T}\mathcal{X}^{(2)}$,
see Section 6.5 in~\cite{shawe-taylor04kernel}.


\section{Hub Language Based CCA Extension}\label{chap:crosslingual:hublang}
Building cross-lingual similarity models based on comparable corpora is challenging for
two main reasons. The first problem is related to missing alignment data: when a number
of languages is large, the dataset of documents that cover all languages is small (or may
even be empty). Even if only two languages are considered, the set of aligned documents
can be small (an extreme example is given by the Piedmontese and Hindi Wikipedias where
no inter-language links are available), in which case none of the methods presented so
far are applicable.

The second challenge is scale - the data is high-dimensional (many languages with
hundreds of thousands of features per language) and the number of multilingual documents may
be large (over one million in case of Wikipedia). The optimization problem posed by CCA is not trivial
to solve: the covariance matrices themselves are prohibitively
large to fit in memory (even storing a 100,000 by 100,000 element matrix requires
80GB of memory) and iterative matrix-multiplication based approaches to solving generalized
eigenvalue problems are required (the covariance matrices can be expressed as products
of sparse matrices, which means we have fast matrix-vector multiplication).

We now describe an extension of CCA to more than two languages, which can be trained on
large comparable corpora and can handle missing data. The extension we consider is based on
a generalization of CCA to more than two views, introduced in~\cite{Kettenring}, namely
the Sum of Squared Correlations SSCOR, introduced in Equation~\ref{eq:SSCOR} in
Section~\ref{chap:extensions:sumcor}, which we will re-state formally later in this section.
Our approach exploits a certain characteristic of the data, namely the \emph{hub language}
characteristic (see below) in two ways: to reduce the dimensionality of the data and to
simplify the optimization problem. We focus on the SSCOR formulation as opposed to the SCOR
formulation which we explored in the previous chapters, since the Lagrangian problem is easier
to solve (That is under the hub language assumption, which we present next).

\bolded{Hub language characteristic.}
In the case of Wikipedia, we observed that even though the training resources are scarce
for certain language pairs, there often exists indirect training data. By considering
a third language, which has training data with both languages in the pair,  we can use
the composition of learned maps as a proxy. We refer to this third language as a hub language.

A \emph{hub language} is a language with a high proportion of non-empty documents in
$D = \left\{d_1,..., d_{\ell}\right\}$. As we have mentioned, we only focus on multilingual
documents that include at least two languages. The prototypical example in the case of
Wikipedia is English. Our notion of the hub language could be interpreted in the following
way. If a non-English Wikipedia page contains one or more links to variants of the page in
other languages, English is very likely to be one of them. That makes English a hub language.

We use the following notation to define subsets of the multilingual comparable corpus:
let $a(i,j)$ denote the index set of all multilingual documents with non-missing data
for the $i$-th and $j$-th language:
$$a(i,j) = \left\{k~ |~ d_k = (u_1,...,u_m), u_i \neq \emptyset, u_j \neq \emptyset \right\},$$
and let $a(i)$ denote the index set of all multilingual documents with non missing data
for the $i$-th language.

We now describe a two step approach to building a cross-lingual similarity matrix.
The first part is related to LSI and reduces the dimensionality of the data. The second
step refines the linear mappings and optimizes the linear dependence between data.

\bolded{Step 1: Hub language based dimensionality reduction.}
The first step in our method is to project $X_1, \ldots, X_m$ to lower-dimensional spaces
without destroying the cross-lingual structure. Treating the nonzero columns of $X_i$ as
observation vectors sampled from an underlying distribution $\mathcal{X}^{(i)} \in \RR^{n_i}$,
we can analyze the empirical cross-covariance matrices:
$$C^{(i,j)} = \frac{1}{|a(i,j)|-1 }\sum_{\ell \in a(i,j)} (X_i(:,\ell) - c_i)\cdot (X_j(:,\ell) - c_j)^T,$$
where $c_i = \frac{1}{|a_i|} \sum_{\ell \in a(i)}X_i(:,\ell)$. By finding low-rank
approximations of $C^{(i,j)}$ we can identify the subspaces that are
relevant for extracting linear patterns between $\mathcal{X}^{(i)}$ and $\mathcal{X}^{(j)}$.
Let $X_1$ represent the hub language corpus matrix. The LSI approach to finding the subspaces
is to perform the singular value decomposition on the full $N \times N$ covariance matrix
composed of blocks $C^{(i,j)}$. If $|a(i,j)|$ is small for many language pairs (as it is in the
case of Wikipedia), then many empirical estimates $C^{(i,j)}$ are unreliable, which can result
in overfitting. For this reason, we perform the truncated singular value decomposition on the
matrix $C = [C^{(1,2)}  \cdots  C^{(1,m)}] \approx U S V^T$, where
$U \in \RR^{n_1 \times k}, S \in \RR^{k \times k}, V \in \RR^{(\sum_{i=2}^m n_i) \times k}$.
Define $T = [U^T V^T]^T \in \RR^{N \times k}$, which is compatible with the block structure $b = (n_1, \ldots, n_m)$.
Note that the columns of $T^{(1)}$ are orthonormal, but that is not true in general for the other blocks.
We proceed by reducing the dimensionality of each $X_i$ by
setting: $Y_i = T^{(i)T} \cdot X_i$, where $Y_i \in \RR^{k\times s}$. To summarize, the first step
reduces the dimensionality of the data and is based on CL-LSI, but optimizes only the hub language
related cross-covariance blocks.

\bolded{Step 2: Simplifying and solving SSCOR.}
The second step involves solving a generalized version of canonical correlation analysis on the
matrices $Y_i$ in order to find the mappings $P_i$. The approach is based on the sum of
squares of correlations formulation by Kettenring~\cite{Kettenring}, where we consider only
correlations between pairs $(Y_1, Y_i), i >1$ due to the hub language problem characteristic.
We will present the original unconstrained optimization problem, then a constrained formulation
based on the hub language problem characteristic. Then we will simplify the constraints and
reformulate the problem as an eigenvalue problem by using Lagrange multipliers.

The original sum of squared correlations is formulated as an unconstrained problem:
\begin{equation*}
  \begin{aligned}
    & \underset{w_i \in \RR^{k}}{\text{maximize}}
    & & \sum_{i < j}^m  \rho(w_i^T Y_i, w_j^T Y_j)^2.
\end{aligned}
\end{equation*}
We solve a similar problem by restricting $i=1$ and omitting the optimization over non-hub
language pairs. Let $D_{i,i} \in \RR^{k \times k}$ denote the empirical covariance of
$\mathcal{Y}_i$ and $D_{i,j}$ denote the empirical cross-covariance computed based on
$\mathcal{Y}_i$ and $\mathcal{Y}_j$. We solve the following constrained (unit variance
constraints) optimization problem:
\begin{equation}\label{squaredCorHubOriginal}
  \begin{aligned}
    & \underset{w_i \in \RR^{k}}{\text{maximize}}
    & & \sum_{i = 2}^m  \left(w_1^T D_{1,i} w_i \right)^2
    & \text{subject to}
    & & w_i^T D_{i,i} w_i = 1, \quad\forall i = 1,\ldots, m.
\end{aligned}
\end{equation}
The constraints $w_i^T D_{i,i} w_i$ can be simplified by using the Cholesky decomposition
$D_{i,i} = K_i^T \cdot K_i$ and substitution: $y_i := K_i w_i$. By inverting the $K_i$
matrices and defining  $G_i := K_1^{-T} D_{1,i} K_i^{-1}$, the problem can be reformulated:
\begin{equation}\label{squaredCorHub}
  \begin{aligned}
    & \underset{y_i \in \RR^{k}}{\text{maximize}}
    & & \sum_{i = 2}^m  \left(y_1^T G_{i} y_i \right)^2
    & \text{subject to}
    & & y_i^T y_i = 1, \quad\forall i = 1,\ldots, m.
\end{aligned}
\end{equation}
A necessary condition for optimality is that the derivatives of the Lagrangian vanish.
The Lagrangian of (\ref{squaredCorHub}) is expressed as:
$$  L(y_1, \ldots, y_m, \lambda_1, \ldots, \lambda_m) =
\sum_{i = 2}^m  \left(y_1^T G_{i} y_i \right)^2 + \sum_{i=1}^m \lambda_i \left(y_i^T y_i - 1\right).$$
Stationarity conditions give us:
\begin{eqnarray}
 \label{dLdx1}  \frac{\partial}{\partial y_1} L = 0 & \Rightarrow & \sum_{i = 2}^m  \left(y_1^T G_{i} y_i \right) G_i y_i + \lambda_1 y_1 = 0, \\
 \label{dLdxi} \frac{\partial}{\partial y_i} L = 0 & \Rightarrow & \left(y_1^T G_{i} y_i \right) G_{i}^T y_1 + \lambda_i y_i = 0,~i > 1.
\end{eqnarray}
Multiplying each Equation (\ref{dLdxi}) with $y_i^T$ and applying the
constraints, we can eliminate $\lambda_i$ which gives us:
\begin{equation}\label{eqy1yi}
G_{i}^T y_1 = \left(y_1^T G_{i} y_i \right) y_i,~i > 1.
\end{equation}
Plugging this into Equation (\ref{dLdx1}), we obtain an eigenvalue problem:
$$\left( \sum_{i = 2}^m G_i G_{i}^T \right) y_1 + \lambda_1 y_1 = 0.$$
The eigenvectors of $\left( \sum_{i = 2}^m G_i G_{i}^T \right)$ solve
the problem for the first language. The solutions for $y_i$ are obtained
from (\ref{eqy1yi}): $y_i := \frac{G_{i}^T y_1}{\| G_{i}^T y_1 \|}$.
Note that the solution (\ref{squaredCorHubOriginal}) can be recovered
by: $w_i := K_i^{-1} y_i$. The original variables $w$
are then expressed as:
\begin{eqnarray}
Y_1 & := & \text{eigenvectors of}~\sum_{i = 2}^m G_i G_{i}^T,\\
W_1 &  = & K_1^{-1} Y_1,\\
W_i &  = & K_i^{-1} G_{i}^T Y_1 N,
\end{eqnarray}
where $N$ is a diagonal matrix that normalizes $G_{i}^T Y_1$, with
$N(j,j) := \frac{1}{\|G(_{i} Y_1(:,j)\|}$.

\bolded{Remark.} The technique is related to  Generalization of Canonical
Correlation Analysis (GCCA)~\cite{Carroll}, where an unknown
group configuration variable is defined and the objective is to maximize the
sum of squared correlations between the group variable and the others. The
problem can be reformulated as an eigenvalue problem. The difference lies in
the fact that we set the unknown group configuration variable as the hub language,
which simplifies the solution. The complexity of our method is $O(k^3)$, where $k$
is the reduced dimension from the LSI preprocessing step, whereas solving the
GCCA method scales as $O(s^3)$, where $s$ is the number of samples (see~\cite{gifi}).
Another issue with GCCA is that it cannot be directly applied to the case of missing documents.

To summarize, we first reduced the dimensionality of our data to $k$-dimensional
features and then found a new representation (via linear transformation) that
maximizes directions of linear dependence between the languages. The final
projections that enable mappings to a common space are defined as:
$P_i(x) := W_i^T T^{(i)T} x.$

\bolded{Remark on SUMCOR.}
The original sum of correlations is formulated as an unconstrained problem:
\begin{equation*}
  \begin{aligned}
    & \underset{w_i \in \RR^{k}}{\text{maximize}}
    & & \sum_{i < j}^m  \rho(w_i^T Y_i, w_j^T Y_j).
\end{aligned}
\end{equation*}
Under the hub language assumption, this problem is equivalent to (analogous to how we derived the formulation in Equation \ref{squaredCorHub}):
\begin{equation}\label{sumCorHub}
  \begin{aligned}
    & \underset{y_i \in \RR^{k}}{\text{maximize}}
    & & \sum_{i = 2}^m  y_1^T G_{i} y_i \\
    & \text{subject to}
    & & y_i^T y_i = 1, \quad\forall i = 1,\ldots, m.
\end{aligned}
\end{equation}
The Lagrangian of (\ref{sumCorHub}) is expressed as:
$$  L(y_1, \ldots, y_m, \lambda_1, \ldots, \lambda_m) =
\sum_{i = 2}^m  y_1^T G_{i} y_i + \sum_{i=1}^m \lambda_i \left(y_i^T y_i - 1\right).$$
Stationarity conditions give us:
\begin{equation}
\begin{aligned} \label{SUMCORdLdx}
  \frac{\partial}{\partial y_1} L = 0 \quad \Rightarrow \quad  &~ \sum_{i = 2}^m  G_i y_i + \lambda_1 y_1 =  0,& \\
  \frac{\partial}{\partial y_i} L = 0 \quad \Rightarrow \quad &~ G_{i}^T y_1 + \lambda_i y_i = 0,&\quad i > 1. \\
\end{aligned}
\end{equation}
%Multiplying Equations (\ref{SUMCORdLdx}) by $y_i$ and using the constraints one can express $\lambda_i$ as:
%\begin{equation}
%\begin{aligned}
%& \lambda_1 & = & - \sum_{i = 2}^m  y_1^T G_i y_i, &\\
%& \lambda_i & = & - y_i^T G_{i}^T y_1, &\quad i > 1.\\
%\end{aligned}
%\end{equation}
It follows that:
$$
G_i y_i = -\frac{1}{\lambda_i} G_i G_i^T y_1,
$$
and thus:
$$
\sum_{i = 2}^m -\frac{1}{\lambda_i} G_i G_i^T y_1 + \lambda_1 y_1 = 0.
$$
Since the following equality can be derived:
$$\lambda_1 = \sum_{i = 2}^m \lambda_i$$
the original problem reduces to
$$
\sum_{i = 2}^m -\frac{1}{\lambda_i} G_i G_i^T y_1 + \sum_{i=2}^m \lambda_i y_1 = 0,
$$
but the solution to that problem does not seem obvious. This is in contrast to the SSCOR problem, where all $\lambda_i$ as
well as $y_i$ could be eliminated simultaneously.

\vspace{5mm}
\bolded{Summary.}
This chapter presented the problem of computing cross-lingual document similarities and then presented four computational
approaches to solve the problem. The last one is an original contribution of the author and is based on combining an efficient
preprocessing step with a particular generalization of CCA (SSCOR), which under an additional assumption (hub language) can
be solved efficiently (both the preprocessing step as well as the simplifying assumption are crucial for the approach). The
next chapter will discuss how the cross-lingual similarity function can be used in a large scale cross-lingual news analysis system. 