%--------------------------------------------------------------------------------------------------
%
\chapter{Conclusions}\label{chap:conclusions}
%--------------------------------------------------------------------------------------------------

\section{Discussion}

%complexity, extension of local method, 
In the thesis we study a generalization of CCA to more than two
sets of variables. We present a new result that proves that
the complexity of the \textbf{SUMCOR problem
is NP-hard} and describe a \textbf{novel approach} to finding \textbf{several sets
of nonlinear patterns}, based on a locally convergent method.
Experimentally, we observe that the
performance of the local method (with linear convergence) is
generally good, although we identify problem settings it gets
stuck in local optima (based on synthetic experiments we observed, 
that increasing the number of views increases the likelihood of such events).
%new relaxation and bounds
We present a \textbf{novel SDP relaxation} of the problem, which can be used to obtain new
local solutions and to provide several \textbf{new computationally tractable bounds} on
global optimality of the SUMCOR problem solutions.
%asses the bounds
The usefulness of the bounds is explored on synthetic problem
instances and a problems related to cross-lingual text-mining.
%novel preprocessing step
We introduce a \textbf{new preprocessing step based on random
projections} to reduce the dimensionality of high dimensional problems
such as in document corpora, making memory requirements tractable.
We demonstrate the applicability of the approach on high-dimensional
text data.

%novel formulations and application
We present an \textbf{application} of two generalizations of CCA, the
SUMCOR and SSCOR formulations to \textbf{cross-lingual similarity function
learning}. The cross-lingual similarity functions are \textbf{applied} to
the task of \textbf{cross-lingual cluster linking}, where we present and evaluate a novel
approach that combines features based on semantic and language analysis.
The approach is shown to be scalable both in
terms of number of articles and number of languages, while accurately linking events.
The approach is used in a system for real-time monitoring of global news in
multiple languages, where a strong server is used to compute two millions
of similarities per second.

%cluster linking vs mate retrieval
On the task of mate retrieval, we observe that refining the LSI-based
projections with hub CCA leads to improved retrieval precision (hub CCA
achieves 0.7 AMPR, whereas LSI achieves 0.6, when 500 dimensions are used), but the
methods perform comparably on the task of event linking (see Table~\ref{table:linkingEvalAlgos}). 
Further inspection showed that the CCA-based approach reached a higher precision on smaller
clusters. The interpretation is that the linking features are highly
aggregated for large clusters, which compensates the lower per-document
precision of LSI. Another possible reason is that the advantage that we
show on Wikipedia is lost on the news domain. This hypothesis could be
validated by testing the approach on documents from a different domain.

%cluster linking exp
The experiments show that the hub CCA-based features present a good baseline,
which can greatly benefit from additional semantic-based features (an increase
in $F_1$ score from 0.78 to 0.88, see Table~\ref{table:linkingEval}).
Even though in our experiments the addition of CCA-based features to semantic features did not
lead to great performance improvements (marginal increase in performance when compared
to concept based features, see Table~\ref{table:linkingEval}), there are two important benefits in the
approach. First, the linking process can be sped up by using a smaller set of
candidate clusters. Second, the approach is robust to languages where semantic
extraction is not available, due to scarce linguistic resources.
For example in Tables~\ref{table:train_test}~and~\ref{table:retrieval} we demonstrate
that that similarity models based on our \textbf{SSCOR reformulation under the hub language
assumption} can be built for language pairs with scarce and
low quality linguistic resources. For example, the 0.85 retrieval score
for the Piedmontese-Hindi pair reported in Table~\ref{table:retrieval} is
promising, since their bilingual training set was empty.

\section{Future Work}

Regarding the work on the SUMCOR formulation, the experiments indicate
that the noisy 1-dimensional embeddings present difficulties for the Horst
algorithm, which is in contrast to the performance on random generic problem instances. A natural
question is, are there other problem structures that result in suboptimal behavior of the local approach?

Our empirical results focus on text data, and an interesting direction is to extend the analysis to
data from other modalities, such as images, sensor streams and graphs.

Also of interest is the complexity analysis of the other generalizations proposed in \cite{Kettenring}.

Regarding to the work on cluster linking, the proposed cross-lingual analysis approaches represent an important building block in our
approach to cross-lingual cluster linking. The language component is
built independently from the cluster linking component.
It is possible that better embeddings can be obtained by methods that
jointly optimize a classification task and the embedding.

Another point of interest is to evaluate our approach to cluster-linking on languages with scarce
linguistic resources, where semantic annotation might not be available. For this purpose,
the labelled dataset of linked clusters should be extended first. The mate retrieval evaluation
shows that even for language pairs with no training set overlap, the hub CCA recovers some signal.

In order to further improve the performance of the classifier for cluster linking, additional features should also
be extracted from articles and clusters and checked if they can increase the accuracy of the classification.
Since the amount of linguistic resources vary significantly from language to language it would also make sense
to build a separate classifier for each language pair. Intuitively, this should improve performance since weights
of individual learning features could be adapted to the tested pair of languages.
