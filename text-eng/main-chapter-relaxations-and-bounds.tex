%--------------------------------------------------------------------------------------------------
%
\chapter{Relaxations and Bounds}\label{chap:relaxations}
%--------------------------------------------------------------------------------------------------
The current chapter focuses on the optimization aspect of the SUMCOR problem.
We present our results on the computational complexity of the
SUMCOR problem formulation, as well as several global optimality bounds.

\section{NP-Hardness}\label{chap:relaxations:nphard}
In this section, we prove that the optimization problem~\ref{eq:qcqp} is not only
non-convex but NP-hard in general. We present a reduction from a general binary quadratic
optimization problem.

Let $A \in \RR^{m\times m}$.
The binary quadratic optimization problem (BQO) is stated as:
\begin{equation}\label{eq:binqp}
\tag{BQO}
\begin{aligned}
& \underset{x \in \RR^m}{\text{max}}
& & x^T A x  \\
& \text{subject to}
& & x\left(i\right)^2 = 1, \quad\forall i =1,\ldots,m.\\
\end{aligned}
\end{equation}
Many hard combinatorial optimization problems (e.g. the
maximum cut  problem and the maximum clique problem~\cite{Garey:1990:CIG:574848})
can be reduced to BQO \cite{Goemans95improvedapproximation}, which is known to be NP-hard.

We reduce a general instance of a  BQO problem to an
instance of the problem (\ref{eq:qcqp}). That means that despite the special
structure of the problem (\ref{eq:qcqp}) (maximizing a positive-definite quadratic
form over a product of spheres), it still falls into the class of problems that
are hard (under the assumption that $P \neq NP$).
We begin with a general instance of BQO and through a set of simple transformations, obtain a specific
instance of (\ref{eq:qcqp}), with a block structure $b = \left(1,\ldots,1\right)$.
More concretely, we will show that any BQO is equivalent to a BQO problem whose assoicated
matrix is a correlation matrix (positive definite with ones on the diagonal), which is an
instance of a the problem (\ref{eq:qcqp}.

Consider a BQO with a corresponding generic matrix $A \in \RR^{m\times m}$. Since $x^T A x = x^T
\frac{\left(A + A^T\right)}{2} x$, we can assume that the matrix $A$ is
symmetric. The binary constraints imply that for any diagonal
matrix $D$ the quantity $x^T D x = \sum_i D\left(i,i\right)$ is
constant. This means that for $c > 0$ large enough, we can
replace the objective with an equivalent objective $x^T \left(A + c
\cdot I\right) x$ which is a positive-definite quadratic form. Setting $c := \norm{A}_1 + 1$
guarantees that $A + c\cdot I$ is positive definite, since it is strictly diagonally dominant.
From now on, we assume that the matrix $A$ in the BQO is symmetric and
positive-definite.  Let $g = \max_i{A\left(i,i\right)}$ and let $D \in
\RR^{m\times m}$ be the diagonal matrix with elements
$D\left(i,i\right) = g - A\left(i,i\right)$. The BQO is then equivalent to
\begin{equation}\label{eq:binqpcor}
\begin{aligned}
& \underset{x \in \RR^m}{\text{max}}
& & x^T \frac{\left(A + D\right)}{g} x  \\
& \text{subject to}
& & x\left(i\right)^2 = 1,  \quad\forall i =1,\ldots,m.\\
\end{aligned}
\end{equation}
The matrix $\frac{\left(A + D\right)}{g}$ is a correlation matrix since it
is a symmetric positive-definite with all diagonal entries
equal to $1$. The optimization problem corresponds to a problem
of maximizing a sum of pairwise correlations between univariate
random variables (using block structure notation: $b\left(i\right) = 1,
\forall i = 1,\ldots, m$). This shows that even the simple case
of maximizing the sum of correlations where the optimal axes are
known and only directions need to be determined, is NP-hard.

With a fixed number of views, the complexity is polynomial, which follows from
 recent results on the computational complexity of quadratic maps \cite{Grigoriev}. The degree of the polynomial is asymptotically
equal to the product\footnote{The number of local solutions to our problem of
interest is established in \cite{Chu}.} $\prod_{i=1}^m n_i$, where $n_i$ is
the dimensionality of the $i$-th view. In many applications (text mining, fMRI
analysis) where the $n_i$'s are large, the computational cost becomes prohibitive
even with $m=3$.

\section{Semidefinite Programming Relaxation}\label{chap:relaxations:sdp}
The Horst's algorithm is scalable and often works well in
practice (see Chapter~\ref{chap:experiments}).
However, it may not converge to a globally optimal
solution. We show how to use a relaxation of the problem to obtain
candidate solutions for the original problem. The relaxation
transforms the problem into a  semidefinite program and we
prove  lower bounds which relates the extracted SDP solution
quality and the optimal QCQP objective value. We also present
a set of upper bounds on the optimal QCQP objective value in Section~\ref{chap:relaxations:upperbounds}.
These can serve as certificates of optimality (or closeness to
optimality) for the local solutions (obtained by the local iterative approach for example).

In this section, we relax the formulation \ref{eq:qcqp} to an SDP problem.
Let $A \in \sym_{++}^{N \times N}$ and $B_1, \ldots, B_m \in \S_{+}^{N \times N}$ be matrices
which share the block structure $b := \left(n_1, \ldots, n_m\right), \sum_i b\left(i\right) = N$.
The blocks $B_i^{(k,l)} \in \RR^{n_k \times n_l}$ for $i,k,l = 1,\ldots,m$ are defined as:
$$
B_i^{(k,l)} := \left\{
     \begin{array}{l l}
       I_{n_i} & : k = i, l = i\\
       0_{k,l} & : \textnormal{otherwise}
     \end{array}
   \right.,
$$
where $I_{n_i} \in \RR^{n_i \times n_i}$ is an identity matrix and $0_{k,l} \in \RR^{k \times l}$ is
a matrix with all entries equal to zero. Since $x^T A x = \mathrm{Tr}\left( A x x^T\right)$, (\ref{eq:qcqp})
can be rewritten as:

\begin{equation}\label{eq:qcqp2}
\tag{QCQP2}
\begin{aligned}
& \underset{x \in \RR^n}{\text{max}}
& &\mathrm{Tr}\left(A x x^T\right) \\
& \text{subject to}
& &\mathrm{Tr}\left(B_i x x^T\right) = 1,  \quad\forall i =1,\ldots,m.\\
\end{aligned}
\end{equation}

Substituting the matrix $x x^T$ with a positive semidefinite matrix $X \in \sym_{+}^{n}$ constrained to have rank one:

\begin{equation*}
\begin{aligned}
& \underset{X \in \sym_{+}^n}{\text{maximize}}
& &\mathrm{Tr}\left(A X\right) \\
& \text{subject to}
& &\mathrm{Tr}\left(B_i X\right) = 1,  \quad\forall i =1,\ldots,m\\
& & &\mathrm{rank}\left(X\right) = 1.
\end{aligned}
\end{equation*}

Omitting the rank-one constraint, we obtain a semi-definite program in standard form:

\begin{equation}\label{eq:sdp}
\tag{SDP}
\begin{aligned}
& \underset{X \in \sym_{+}^n}{\text{max}}
& &\mathrm{Tr}\left(A X\right) \\
& \text{subject to}
& &\mathrm{Tr}\left(B_i X\right) = 1,  \quad\forall i =1,\ldots,m.\\
\end{aligned}
\end{equation}

\bolded{Remark.}
If the solution of the problem (\ref{eq:sdp}) is rank-one, i.e. $X$ can be expressed as $X = y \cdot y^T$,
then $y$ is the optimal solution for (\ref{eq:qcqp}).

\noindent\textbf{Low Rank Solutions}
We use the solutions of the SDP relaxation to extract solutions to
the original QCQP problem. In the process, we obtain a bound which relates
the global SDP bound and the optimal value of QCQP, giving a measure of
the quality of the extracted solution. If the solution is
rank-one, then the relaxation is exact. Here we consider the case where the solutions are
 low rank (close to rank-one).

Let $X^*$ be a solution to (\ref{eq:sdp}) and $x^{*}$ be the solution
to (\ref{eq:qcqp}).
A straightforward way to extract a feasible solution to the problem
(\ref{eq:qcqp}) from $X^*$ is to project its leading eigenvector to
the set of constraints.  The following inequality always holds:
 $$\mathrm{Tr}\left(A X^{*}\right) \geq \mathrm{Tr}\left(A \cdot x^{*} \cdot x^{*T}\right).$$
The quality of the solution depends on how loose this inequality is,
or rather how close the matrix $X^*$ is to rank-one (Proposition~\ref{thm:rank2sdp_solution}).
These depend on the spectral properties of matrix
$X$ and matrix $A$.

The projection of a vector $y \in \RR^N, \norm{y^{(i)}} \neq 0$ to the
feasible set of (\ref{eq:qcqp}) is given by map $\pi\left(\cdot\right) : \RR^N \rightarrow \RR^N$:
$$\pi\left(y\right) := \left(\frac{y^{(1)}}{\norm{y^{(1)}}}, \ldots, \frac{y^{(m)}}{\norm{y^{(m)}}}\right)^T.$$

We require the following technical assumption:
\begin{assumption}\label{thm:assumption1}
Let $b = \left(n_1,\ldots,n_m\right)$ denote the block structure and $ \sum_i n_i = N $.
Let $X^*$ be the solution to the problem (\ref{eq:sdp}). Let $x_k$ denote the $k$-th
eigenvector of $X^*$. The assumption is the following:
$$\norm{x_1^{(i)}} > 0, \forall i = 1,\ldots, m.$$
\end{assumption}

\begin{remark} The assumption ensures that the projection to the feasible set,
$\pi(\cdot)$, is well defined. In our experiments, this was always
the case, but does not hold in general as one can find counterexamples, for example, let $m = 2$ and define:
$$
A = \begin{bmatrix}
  1 & 0 \\
  0 & 1
 \end{bmatrix},
B_1 = \begin{bmatrix}
  1 & 0 \\
  0 & 0
 \end{bmatrix},
B_2 = \begin{bmatrix}
  0 & 0 \\
  0 & 1
 \end{bmatrix}.
$$
All symmetric positive definite matrices that satisfy the constraints are of the form:
$$
X = \begin{bmatrix}
  1 & \epsilon \\
  \epsilon & 1
  \end{bmatrix},
$$ where $\epsilon < 0$, and attain the same value $\mathrm{Tr}\left(A X \right) = 2$, which is thus optimal.
Since this also holds for $X = I$ where the leading eigenvector can be defined as $x_1 = [0 1]^T$, which is in
contrast with the assumption. This counterexample has a specific structure: the zeros on the off-diagonal of $A$
mean that the views are completely independent and the problem is in some sense a trivial multi-view problem.
It is possible that the assumption holds under additional conditions, for example, that the
matrix is not block diagonalizable under the problem's block structure. 
  
\end{remark}


The following lemma depends on the projection
operator and thus relies on the assumption.

\begin{lemma}
Let $b = \left(n_1,\ldots,n_m\right)$ denote the block structure and $ \sum_i n_i = N $.
Let $X^*$ be the solution to (\ref{eq:sdp}) for which Assumption \ref{thm:assumption1} holds.
Let $\alpha_i := \frac{1}{\norm{x_1^{(i)}}}$.
If $X^*$ can be expressed as:
$$X^* = \lambda_1  x_1 x_1^T + \lambda_2 x_2 x_2^T,$$
where $\norm{x_1}=1$, $\norm{x_2}=1$, $\langle x_1, x_2 \rangle$ and $\lambda_1 > 1 >  \lambda_2$, then
$$\lambda_1 \leq \alpha_i \alpha_j  \leq \frac{\lambda_1}{1 - \lambda_2}.$$
\end{lemma}

\begin{proof}
The constraints in problem (\ref{eq:sdp}) are equivalent to:
$$\lambda_1 \norm{x_1^{(i)}}^2 + \lambda_2 \norm{x_2^{(i)}}^2 = 1, \forall i = 1,\ldots, m.$$
Since $\lambda_2 < 1$ and $\norm{x_2^{(i)}}^2 \leq 1$,
we see that $0 \leq \lambda_2 \norm{x_2^{(i)}}^2  < 1$.
Consequently:
$$  0 < \frac{1- \lambda_2}{\lambda_1} < \norm{x_2^{(i)}}^2 \leq \frac{1}{\lambda_1}.$$
From $\alpha_i = \frac{1}{x_1^{(i)}}$ it follows that
$$ \sqrt{\lambda_1} \leq \alpha_i \leq \sqrt{\frac{\lambda_1}{1-\lambda_2}},$$
and finally:
$$ \lambda_1 \leq \alpha_i \alpha_j \leq \frac{\lambda_1}{1-\lambda_2}, \forall i,j = 1,\ldots,m.$$
\end{proof}

\begin{proposition}\label{thm:rank2sdp_solution}
Let $b = \left(n_1,\ldots,n_m\right)$ denote the block structure and $ \sum_i n_i = N $.
Let $X^*$ be the solution to (\ref{eq:sdp}) and $x^{*}$ be the solution to  (\ref{eq:qcqp}).
Let $x_k$ denote the $k$-th eigenvector of $X^*$, $\alpha_i := \frac{1}{\norm{x_1^{(i)}}}$,
$\psi := \mathrm{Tr}\left(A X^{*}\right)$, and $\phi := \mathrm{Tr}\left(A \cdot x^{*} \cdot x^{*T}\right)$.
If $X^*$ can be expressed as
$$X^* = \lambda_1  x_1 x_1^T + \lambda_2 x_2 x_2^T,$$
where $x_1$ and $x_2$ have unit length and $\lambda_1 > 1 >  \lambda_2$,
 then: $$\psi - \pi\left(x_1\right)^T A \pi\left(x_1\right) \leq \left(\frac{1}{1-\lambda_2} -1\right)  m^2 + \lambda_2 \norm{A}_2.$$
\end{proposition}

\begin{proof}
First we note that: $$\psi \geq \phi \geq \pi\left(x_1\right)^T A \pi\left(x_1\right).$$
We can expand the left hand side in terms of the two eigenvectors. Grouping the
elements by the vector terms and using basic manipulations we arrive at the result.
\[
\begin{array}{rcl}
\psi - \pi\left(x_1\right)^T A \pi\left(x_1\right) & = & \lambda_1 \sum_{i,j} x_1^{(i)T} A^{(i,j)} x_1^{(j)T}  \\
& & + \lambda_2\sum_{i,j} x_2^{(i)T} A^{(i,j)} x_2^{(j)T} - \sum_{i,j} \alpha_i \alpha_j x_1^{(i)T} A^{(i,j)} x_1^{(j)T}   \\
&\leq & \sum_{i,j} (\lambda_1 - \alpha_i \alpha_j) x_1^{(i)T} A^{(i,j)} x_1^{(j)T}  + \lambda_2 \norm{A}_2 \\
&\leq & -\sum_{i,j} (\lambda_1 - \alpha_i \alpha_j)\cdot |x_1^{(i)T} A^{(i,j)} x_1^{(j)T}| +  \lambda_2 \norm{A}_2 \\
&\leq & \left(\frac{\lambda_1}{1-\lambda_2} - \lambda_1 \right)\cdot \sum_{i,j}  \frac{1}{\alpha_i \alpha_j} +  \lambda_2 \norm{A}_2 \\
&\leq & \left(\frac{\lambda_1}{1-\lambda_2} - \lambda_1 \right)\cdot \frac{m^2}{\lambda_1} +  \lambda_2 \norm{A}_2 \\
&= & \left(\frac{1}{1-\lambda_2} - 1 \right)\cdot m^2 +  \lambda_2 \norm{A}_2.
\end{array}
\]
\end{proof}

\begin{remark}
Proposition~\ref{thm:rank2sdp_solution} is based on the assumption that the solution is of rank 2. Although the assumption is very specific, it holds in general
for $m=3$ which is a result of Theorem~\ref{sdpRankTheorem}.
\end{remark}

A similar bound can be derived for the general case, provided that the solution to problem (\ref{eq:sdp}) is close to rank-one.
\begin{proposition}
Let $b = \left(n_1,\ldots,n_m\right)$ denote the block structure and $ \sum_i n_i = N $.
Let $X^*$ be the solution to the problem (\ref{eq:sdp}) and $x^{*}$ the solution to the problem (\ref{eq:qcqp}).
Let $x_k$ denote the $k$-th eigenvector of $X^*$,
$\alpha_i := \frac{1}{\norm{x_1^{(i)}}}$ and
$\psi := \mathrm{Tr}\left(A X^{*}\right)$.
If $X^*$ can be expressed as
$$X^* = \lambda_1  x_1 x_1^T + \lambda_2 x_2 x_2^T + \cdots + \lambda_n x_n x_n^T,$$
where each $x_i$ has unit length and $\lambda_1 > 1 >  \underset{i=2,\ldots, n}{\sum} \lambda_i$,
 then:
\begin{align*}
\psi &- \pi\left(x_1\right)^T A \pi\left(x_1\right) \\ &\leq  \left(\frac{1}{1- \underset{i=2,\ldots, n}{\sum} \lambda_i} -1\right)  m^2 + \left(\underset{i=2,\ldots, n}{\sum} \lambda_i\right) \norm{A}_2.
\end{align*}
\end{proposition}

\section{Upper Bounds on QCQP}\label{chap:relaxations:upperbounds}
We now present several upper bounds on the optimal QCQP objective
value based on the spectral properties of the QCQP matrix $A$. We then bound the
values of the SDP solutions and present two constant relative accuracy
guarantees.

\noindent\textbf{$\mathbf{L_2}$ Norm Bound}
We show an  upper bound on the objective of (\ref{eq:qcqp}) based on the largest eigenvalue
of the problem matrix $A$.

\begin{proposition}
The objective value of (\ref{eq:qcqp}) is upper bounded by $m \cdot \norm{A}_2$.
\end{proposition}
\begin{proof}
The problem (\ref{eq:qcqp}) remains the same if we add a redundant constraint $x^T x = m$
obtained by summing the constraints $\sum_{i= 1}^{m} \left(x^{(i)T}x^{(i)} - 1\right) = 0$.
We then relax the problem by dropping the original constraints to get:
\begin{equation}\label{eq:evp}
\begin{aligned}
& \underset{x \in \RR^{N}}{\text{max}}
& & x^T A x\\
& \text{subject to}
& &x^{T} x = m.
\end{aligned}
\end{equation}
Since $\norm{A}_2 = \text{max}_{\norm{x}_2 = 1} x^T A x$, it follows that the optimal
objective value of the problem (\ref{eq:evp}) equals $m \cdot \norm{A}_2 $.
\end{proof}

\noindent\textbf{Bound on possible SDP objective values}
The next lemma establishes two simple bounds on the optimal value of the SDP problem.
\begin{lemma}\label{eq:lemsdp}
Let $X^*$ be the solution to the problem (\ref{eq:sdp}) and let $\psi := \mathrm{Tr}\left(A X^{*}\right)$. Then
$$m \leq \psi \leq m^2.$$
\end{lemma}
\begin{proof}
Express $X^*$ as:
$$X^* =  \underset{i=1,\ldots, N}{\sum} \lambda_i x_i x_i^T,$$
where each $x_i$ has unit length and $\lambda_1 \geq \ldots \geq \lambda_N \geq 0$.
The lower bound follows from the fact that $\psi$ upper bounds the optimal objective
value of problem (\ref{eq:qcqp}) which is lower bounded by $m$. The lower bound corresponds
to the case of zero sum of correlations.

To prove the upper bound, first observe that the constraints in (\ref{eq:sdp})
imply that $\underset{i=1,\ldots, N}{\sum}\lambda_i = m$.
Let $y \in \RR^{N}$ and $\norm{y} = 1$.
Let $z := \left(\norm{y^{(1)}}, \ldots, \norm{y^{(m)}}\right)^T.$
Observe that $\norm{z} = 1$ and that $\norm{z z^T}_2 = 1$.
Define $e \in \RR^m, e\left(i\right) = 1,  \forall i=1,\ldots,m$.
We now bound $\norm{A}_2$:
\begin{align*}
  y^T A y &= \sum_{i, j = 1,\ldots,m} y^{(i)T} A^{(i,j)} y^{(j)} = \\
& = \sum_{i, j = 1,\ldots,m} \norm{y^{(i)}} \norm{y^{(j)}} \frac{y^{(i)T}}{\norm{y^{(i)}}} A^{(i,j)} \frac{y^{(j)}}{\norm{y^{(j)}}} \leq \\
&\leq \sum_{i, j = 1,\ldots,m} \norm{y^{(i)}} \norm{y^{(j)}} = \\
&= e^T (z z^T) e \leq \norm{e}\cdot \norm{z z^T}_2 \cdot \norm{e} = m.
\end{align*}
We used the fact that $\frac{y^{(i)T}}{\norm{y^{(i)}}} A^{(i,j)} \frac{y^{(j)}}{\norm{y^{(j)}}}$
is a correlation coefficient and thus bounded by $1$.
The upper bound follows:
$$\mathrm{Tr}\left(A X^{*}\right) = \mathrm{Tr}\left(A  \underset{i=1,\ldots, N}{\sum} \lambda_i x^{(i)} x^{(i)T}\right) =$$
$$=  \underset{i=1,\ldots, N}{\sum} \lambda_i x^{(i)T} A x^{(i)} \leq \underset{i=1,\ldots, N}{\sum} \lambda_i \cdot m = m^2.$$
\end{proof}

\begin{corollary}
The difference (duality gap) between the optimal objective value of the~\ref{eq:sdp} and its the optimal objective value of
its dual formulation (see Definition~\ref{eq:background:sdp:primal} and Definition~\ref{eq:background:sdp:dual}) is zero.
\end{corollary}
\begin{proof}
This is a consequence of the boundedness of the optimization problem~\ref{eq:sdp} and the fact that a strictly
feasible point exists, for example: $$X_0 := I \in S_{++}^N.$$
The statement follows from Lemma~\ref{eq:lemsdp} and Theorem~\ref{thm:background:sdp_strong}.
\end{proof}

\noindent\textbf{Constant Relative Accuracy Guarantee}
There exists a lower bound on the ratio between the objective values of the original
and the relaxed problem that is independent on the problem dimension. The bound is
based on the following result from \cite{Nesterov98globalquadratic} (Theorem 2.1).
The theorem refers to convex sets in Euclidean spaces \cite{Boyd:2004:CO:993483} and
concepts from general topology (\cite{bourbaki1998general}): closed sets,
bounded sets and set interior.
\begin{notation}
Let $\mathrm{Square}(\cdot)$ denote componentwise squaring: if
$y = \mathrm{Square}(x)$ then $y(i) = x(i)^2$ and let $\mathrm{diag}(X)$
denote the vector corresponding to the diagonal of $X$.
\end{notation}
\begin{definition}
$K \subset \RR^n$ is a \emph{convex cone} if it is closed under conic combinations:
$\alpha x + \beta y \in K$ for all $x \in K, y \in K, \alpha > 0, \beta > 0$.
A cone is \emph{pointed} if it contains the null vector (origin) $0_n$.
\end{definition}

\begin{theorem}[\protect{\cite[page 5]{Nesterov98globalquadratic}}]\label{thm:nesterov}
Let $A \in \RR^{N \times N}$ be symmetric and let $\mathcal{F}$ be a set with the following properties:
\begin{itemize}
\item $\mathcal{F} = \left\{ v \in K: B v = c  \right\},$ where $K$ is a convex closed pointed
cone in $\RR^N$ with non-empty interior, $B \in \RR^{k\times N}$, $c \neq 0_k$, and
$\left\{  v \in \mathrm{int}K : B v = c \right\} \neq \emptyset$.
\item $\mathcal{F}$ is closed, convex and bounded.
\item There exists a componentwise strictly positive $v \in \mathcal{F}$.

\end{itemize}
Denote:
\begin{align*}
\phi^* &=& \mathrm{max}\left\{  x^T A x : \mathrm{square}\left(x\right) \in \mathcal{F}  \right\},\\
\phi_* &=& \mathrm{min}\left\{  x^T A x : \mathrm{square}\left(x\right) \in \mathcal{F}  \right\},\\
\psi^* &=& \mathrm{max}\left\{  \mathrm{Tr}\left(A X\right): \mathrm{diag}\left(X\right) \in \mathcal{F}, X \in \sym_+^N  \right\},\\
\psi_* &=& \mathrm{min}\left\{  \mathrm{Tr}\left(A X\right): \mathrm{diag}\left(X\right) \in \mathcal{F}, X \in \sym_+^N  \right\},\\
\psi \left(\alpha\right) &=& \alpha \psi^* + \left(1-\alpha\right)\psi_*.
\end{align*}

Then
\begin{equation}\label{eq:sdpBound}
\begin{aligned}
\psi_* \leq \phi_* \leq \psi \left(1 - \frac{2}{\pi}\right) \leq \psi\left(\frac{2}{\pi}\right) \leq \phi^* \leq \psi^*.
\end{aligned}
\end{equation}
\end{theorem}

\begin{theorem}
Let
$x^{*}$ be the solution to the problem (\ref{eq:qcqp2}) and
$X^*$ be the solution to the problem (\ref{eq:sdp}).
Let $b = \left(n_1,\ldots,n_m\right)$ denote the block structure where $\sum_i n_i = N$.
Let $\phi^*:= \mathrm{Tr}\left(A \cdot x^{*} \cdot x^{*T}\right)$ and
$\psi^* := \mathrm{Tr}\left(A X^{*}\right)$.
Then $$\frac{2}{\pi} \psi^* \leq \phi^* \leq \psi^*.$$
\end{theorem}

\begin{proof}
First note that it follows from $A \in \sym_{+}^n$ that $\psi_* \geq
0$. This is a consequence of the fact that $\mathrm{Tr}\left(A X \right) \geq 0$, for any $X \in \sym_+^n$
(and therefore for the minimizer $X_*$). The positiveness of the trace can be deduced from:
$\mathrm{Tr}\left(A X \right) = \mathrm{trace}\left(C_A C_A^T C_X C_X^T \right) =
\mathrm{trace}\left(C_X^T C_A C_A^T C_X \right) = \norm{C_A^T C_X}_F^2 \geq 0 $,
where $A = C_A C_A^T$ and $X = C_X C_X^T$ are decompositions of $A$ and $X$, which exist since
symmetric matrices are diagonalizable and the eigenvalues are nonnegative.

We now show that the problems (\ref{eq:qcqp2}) and (\ref{eq:sdp}) can be reformulated
so that the Theorem~\ref{thm:nesterov} applies.

Note that the feasible sets in (\ref{eq:qcqp2}) and
(\ref{eq:sdp}) are defined in terms of equalities. Since the corresponding objective
functions are convex and the feasible sets are closed and bounded in
both cases, the corresponding optima must lie on their borders. For this reason, we can
replace the equality constraints with inequality constraints without loss of generality:
$x^{(i)T}x^{(i)} \leq 1$ in (\ref{eq:qcqp2}) and $\mathrm{Tr} \left(B_i X\right) \leq 1$
in (\ref{eq:sdp}).

Next, we add redundant constraints to the two problems respectively:
$\mathrm{Square}\left(x^{(i)}\right) \geq 0, \forall i = 1,\ldots,m$
and $X\left(j,j\right) \geq 0, \forall j = 1,\ldots, N$.

Define $\widetilde{\mathcal{F}} = \left\{x \in \RR^N | x^{(i)} \in \Delta^{n_i -1} \right\}$,
where $$\Delta^{k} = \left\{ x \in \RR^{k+1} | x\left(i\right) \geq 0, \forall i ~\mathrm{and}~ \sum_i x\left(i\right) = 1 \right\}.$$

$\widetilde{\mathcal{F}}$ is a product of standard simplices: $\widetilde{\mathcal{F}} = \prod_{i = 1}^m \Delta^{n_i -1}$. It follows that the set is closed, bounded and convex.

$\widetilde{\mathcal{F}}$ can be embedded in $\RR^{N+1}$ in order to obtain the desired conic
formulation. We now define the matrices referred to in the theorem:
\begin{align*}
K := \{ t\cdot \left[1~ x^T\right]^T &| t \geq 0, x \in \widetilde{\mathcal{F}} \}, \\
B := \left[1~ 0_N^T\right]^T,\quad c = 1,& \quad \mathcal{F} := K \cap \left\{x | B x = c \right\}.
\end{align*}
\begin{sloppypar}
Define $v = \left[v_1^T \ldots v_m^T\right]^T,$ where $v_i\left(j\right) = \frac{1}{n_i}$.
The vector $\left[1~ v^{T}\right]^T$ is strictly positive and lies in $\mathrm{int}\left(K\right) \cap \left\{x \in \RR^{N+1} | B x = c \right\}$.
Let $\widetilde{A} \in \RR^{N+1}$ be defined as $\widetilde{A}\left(1,i\right) = 0$,
$\widetilde{A}\left(i,1\right) = 0, \forall i$ and $\widetilde{A}\left(i,j\right) = A\left(i-1,j-1\right), \forall i,j > 1$.
\end{sloppypar}
The optimization problem (\ref{eq:qcqp2}) is equivalent (with
 the same optimal objective value) to:
\begin{equation*}
\begin{aligned}
& \underset{x \in \RR^{N+1}}{\text{max}}
& &\mathrm{Tr}\left(\widetilde{A} x x^T\right) \\
& \text{subject to}
& &\mathrm{Square}\left(x\right) \in \mathcal{F}.\\
\end{aligned}
\end{equation*}

The optimization problem (\ref{eq:sdp}) is likewise equivalent to the problem:
\begin{equation*}
\begin{aligned}
& \underset{X \in \sym_{+}^{N+1}}{\text{max}}
& &\mathrm{Tr}\left(\widetilde{A} X\right) \\
& \text{subject to}
& &\mathrm{diag}\left(X\right) \in \mathcal{F}.\\
\end{aligned}
\end{equation*}

Using the definition of $\psi\left(\alpha\right)$ and the fact that $\psi_* \geq 0$ it follows
that $\psi\left(\alpha\right) \geq \alpha \psi^* ,\forall \alpha \geq 0$. Substituting
$\alpha = \frac{2}{\pi}$ in (\ref{eq:sdpBound}) we get the desired result:
$$\frac{2}{\pi} \psi^* \leq \phi^* \leq \psi^*.$$
\end{proof}

Observe that the bound above relates the optimization problems (\ref{eq:qcqp}) and (\ref{eq:sdp})
and not (\ref{eq:qcqp0}) with its SDP relaxation. Let $\widetilde{\phi}$ denote the optimum value
of the objective function in (\ref{eq:qcqp0}) and let $\widetilde{\psi}$ denote the optimum value
of the objective function of the corresponding SDP relaxation. It is easy to see that
$2 \cdot \widetilde{\phi} + m = \phi$ and $2 \cdot \widetilde{\psi} + m = \psi$, which is a
consequence of transformations of the original problems to their equivalent symmetric
positive-definite problems. The $\frac{2}{\pi}$ constant relative accuracy bound becomes a bit
weaker in terms of the original problem and its relaxation. This fact is stated in the following corollary.
\begin{corollary}
The optimum values of the objective function in (\ref{eq:qcqp0}) and its corresponding relaxation,
denoted $\widetilde{\phi}$ and $\widetilde{\psi}$ respectively, are related by:
$$\widetilde{\phi} \geq \frac{2}{\pi} \widetilde{\psi} - \frac{(1 - \frac{2}{\pi}) m}{2}.$$
\end{corollary}

\noindent\textbf{Improved Bound on the Relative Accuracy}
We can exploit the additional structure of the problem to obtain a slightly better bound.
The result is based on applying Theorem 3.1 from \cite{Nesterov98globalquadratic}.
Define
$$\omega\left(\beta\right) := \beta \arcsin\left(\beta\right) + \sqrt{1 - \beta^2}.$$
The function $\omega\left(\beta\right)$ is increasing and convex with
$\omega\left(0\right) = 1$ and $\omega\left(1\right) = \frac{\pi}{2}$.

\begin{theorem}[\protect{\cite[page 7]{Nesterov98globalquadratic}}]\label{thm:nesterov2}
Denote

\begin{align*}
\tau^* &=&  \textnormal{max}\{ \langle \diag(A),v \rangle \colon v \geq, v \in \mathcal{F}  \}, \\ 
\tau_* &=& \textnormal{min}\{ \langle \diag(A),v \rangle \colon v \geq, v \in \mathcal{F}  \}, \\
\beta^* &=& \frac{\psi^* - \tau^*}{\psi^*-\psi_*} \in [0,1], \\
\beta_* &=& \frac{\tau_* - \psi_*}{\psi^*-\psi_*} \in [0,1], \\
\alpha^* &=& \textnormal{max}\{\frac{2}{\pi}\omega(\beta_*), 1 - \beta^* \}, \\
\alpha_* &=& \textnormal{min}\{1 - \frac{2}{\pi}\omega(\beta^*), \beta_* \}.
\end{align*}

The optimal values of the problems in Theorem \ref{thm:nesterov} satisfy the following relations:
\begin{align*}
\psi^* \geq \phi^* \geq \psi(\alpha^*), \\
\psi_* \leq \phi_* \leq \psi(\alpha^*). \\
\end{align*}
\end{theorem}


Applying the Theorem \ref{thm:nesterov2} we obtain the result:
$$ \max\left\{\frac{2}{\pi}\omega\left(\frac{m}{\psi^*}\right), \frac{m}{\psi^*} \right\}   \psi^* \leq \phi^* \leq \psi^*.$$

This results in a minor improvement of the default bound. For example, when $m = 3$ and
the fact that $\frac{m}{\psi^*} \geq \frac{1}{3}$ we obtain the following:
$$ \frac{2}{\pi} \psi^* \leq \frac{105}{100}  \cdot \frac{2}{\pi} \psi^* \leq \phi^* \leq \psi^*.$$


\section{Random Projections and Multivariate Regression}\label{chap:relaxations:practical}

Although SDP problems are convex and admit polynomial time solutions, their
applicability is difficult when the total number of features and the number of
instances are large. For example, the applications of multiview
learning to text analysis typically involve hundreds of thousands variables
and training instances, while typical SDP solvers can find solutions to relaxed forms of
QCQPs with up to a few thousand original variables.

To address this issue, we reduce the dimensionality of the feature vectors, resulting in
tractable SDP problem dimensions.

One way to analyze a general dataset (single view) is to
perform a singular value decomposition of the data matrix as
we introduced in Section~\ref{chap:background:svd}.
A set of singular vectors corresponding to the largest singular
values can be used to project the data into a lower-dimensional
subspace. If computing the basis is too expensive, one can generate a random
larger set of basis vectors that achieve similar reconstruction
errors. However, this random projection basis is not
informative in the same sense as the SVD basis is
(directions extracted by SVD reflect which directions are
prevalent in the data, as opposed to random directions).

When dealing with multiple views, a natural approach to
dimensionality reduction is to compute the TSVD of the multiview stacked matrix (introduced in Chapter~\ref{chap:notation}).
Using random projections to approximate the computation then seems like
a good strategy.

We experimentally observed that the number of random projections needed to approximate
the TSVD decomposition thus construct a multi-view aligned basis is prohibitively large -
while a relatively small number of random projections is needed to capture the variance in view, a large
number of random projections is needed to approximate all the cross-covariance matrices simultaneously.
Generating random subspaces with a fixed $k$ sequentially over each view is problematic, since
the probability of generating a subspace for the $m$-th view that is well correlated to the preceding
views decreases as $m$ increases.

Our approach is based on the following idea. Generate a set of random vectors for one view and use
Canonical Correlation Analysis Regression (CCAR)\cite{ccar} (a method similar to ridge regression)
to find their representatives in the other views. Repeat the procedure for each of the remaining views
to prevent bias to a single view. We hypothesize that restricting our search in the spaces spanned
by the constructed bases still leads to good solutions, which we will demonstrate in Chapter~\ref{chap:experiments}. 
The procedure is detailed in Algorithm~\ref{algorithm:rpgen}.

Let $m$ be the number of vector spaces corresponding to different
views and $n_i$ the dimensionality of the $i$-th vector
space. Let $X^{(i)} \in \RR^{n_i \times N}$ represent the aligned
data matrix for the $i$-th view.


\begin{algorithm}
\caption{Random projections basis generation}
\label{algorithm:rpgen}
{\bf Input:} $X^{(1)},\ldots X^{(m)}$, $\gamma$ - the regularization coefficient, $k$ - \# of projections/block
\begin{algorithmic}
\FOR{$i = 1$ to $m$}
\STATE $P_{(i,i)} :=$ random $n_i \times k$ matrix with elements sampled $i.i.d.$ from standard normal distribution.
\STATE Re-scale each column of $P_{(i,i)}$ so that its norm is equal to $\sqrt{\frac{n_i}{k}}$.
\FOR{$j = 1$ to $m$}
\IF {$j = i$}
 \STATE continue
\ENDIF
\STATE  $\alpha_{(i,j)} :=  \left(\left(1-\gamma\right) X^{(j)} X^{(j)T} + \gamma  I_j \right)^{-1}$
\STATE  $P_{(i,j)} :=  \alpha_{(i,j)} X^{(j)} X^{(i)T}  P_{(i,i)},$ where $I_j$ is the $n_j \times n_j$ identity matrix.
\ENDFOR
\ENDFOR
\\
\end{algorithmic}
{\bf Output:} matrices $P_{(i,j)} \;\text{for}\; i,j = 1,\ldots,m$
\end{algorithm}
\begin{sloppypar}
The matrices $P_{(i,1)}, \ldots, P_{(i,m)}$ form the bases of
vector spaces corresponding to $X^{(1)},\ldots, X^{(m)}$. By using horizontal
matrix concatenation we define the full basis for the $i$-th view:
\begin{equation}\label{eq:rp_projectors}
P_i := \left[P_{(1,i)}, \ldots, P_{(m,i)}\right].
\end{equation}
\end{sloppypar}

\bolded{Remark.}
The Algorithm \ref{algorithm:rpgen} is a heuristic dimensionality reduction step that tries to reduce the dimensionality of the space
without destroying the pairwise correlation structure. In practice we observed that a purely random projection requires a prohibitively 
high dimension in order to capture the highly correlated subspaces. For this reason, starting with a random subspace of a particular view,
we find its highly correlated `'counterparts" in the other views by performing independent pairwise analyses (similar to linear
regression).

\bolded{Remark on SDP optimization.}
Efficiently solving large SDP problems remains an active area 
of research. In this work we relied on an SDP solver implementation of a primal-dual interior point
algorithm, see~\cite{todd2008study}\cite{wang2009new} for on overview and modern results. Many other
approaches exist in the literature: a primal-dual combinatorial approach based on the idea of multiplicative
updates~\cite{arora2007combinatorial}, first-order methods with low memory requirements\cite{lu2007large}, spectral bundle
methods~\cite{helmberg2000spectral} which can exploit specific problem structure, just to name a few.

\vspace{5mm}
\bolded{Summary.}
This chapter discussed the optimization aspects of the SUMCOR generalization. We first presented that the problem is NP-hard in general, which
motivated our results on global optimality bounds. We presented a SDP relaxation of the problem that yields bounds on the global solution
as well as candidate solutions to the original problem under certain assumptions. Although the bound can be computed in polynomial time, 
applying it on a large and high dimensional dataset is a challenge. To address this we presented a heuristic dimensionality reduction
step based on random projections, which significantly reduces the computational cost. This fact which will be demonstrated in Chapter \ref{chap:experiments}
on a high-dimensional dataset.
