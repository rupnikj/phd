%--------------------------------------------------------------------------------------------------
%
\chapter{Background}\label{chap:background}
%--------------------------------------------------------------------------------------------------

The central subjects in the thesis revolve around statistical approaches to finding structure in one, two or more sets of variates. We will
introduce two methods that find structure in a single set of variates: $k$-means clustering and Singular Value Decomposition (SVD) for dimensionality reduction, which is closely related to Principal Component Analysis (PCA). We will then present Canonical Correlation Analysis (CCA), a method for studying two sets of variates. We will also briefly cover kernel method extensions of the methods.


\section{$k$-means Decomposition}\label{chap:background:kmeans}

The $k$-means algorithm~\cite{kmeans} is perhaps the most well-known and widely-used clustering algorithm. In spirit of analysis on multiview methods that is to be presented, we will formulate $k$-means as a matrix factorization problem. Given a $n \times \ell$ sample matrix~\ref{def:notation:sample_matrix}
the goal is to find a best rank $k$ approximation under additional constraints:

\begin{equation}\label{eq:background:kmeans:opt}
\begin{aligned}
& \underset{C \in \RR^{n \times k}, P \in \RR^{\ell \times k}}{\text{minimize}}
& & \norm{X - C \cdot P^T}_F^2, \\
& \text{subject to}
& & P(i,j) \in \{0,1\}, \quad \forall i,j \\
& & & \sum_j P(i, j) = 1, \quad \forall i.
\end{aligned}
\end{equation}

The interpretation of the additional constraints on matrix $P$ is that they force each sample vector 
(column in $X$) to select precisely one column of $C$ to approximate it and the objective function
corresponds to minimizing a sum of squared errors made by approximating points with centroids.

The matrix $C$ in Equation~\ref{eq:background:kmeans:opt} is uniquely defined given $P$, since for any given set
of points in $\RR^n$, the point that minimizes the sum of squared distances to the set is the mean. Since each
column of $P$ selects a subset of columns of $X$, $C$ can be expressed as:
\begin{equation}\label{eq:background:kmeans:centroids}
\begin{aligned}
 C := X\cdot P diag(\vec{1}_\ell^T \cdot P)^{-1} P^T,
\end{aligned}
\end{equation}
where the inverse of the diagonal matrix corresponds dividing by the set size when computing the mean ($\vec{1}_\ell^T \cdot P$ counts
the number of points assigned to each of the $k$ clusters). In addition, given $C$, the assignment $P$ that
minimizes the sum of squared errors can be found by:
\begin{equation}\label{eq:background:kmeans:assignments}
\begin{aligned}
P(i, j^*) = 1,\quad \textnormal{where}\quad j^* = argmin_j \norm{X(:,i) - C(:,j)}
\end{aligned}
\end{equation}

A popular approach~\cite{kmeans} to solving~\ref{eq:background:kmeans:opt} is to start with an initial assignment and alternate between updating $C$ given $P$ and vice
versa. The approach is widely used in practice, even though it is susceptible to finding local minima. In general, the problem is known to be NP-hard~\cite{aloise2009np}.

\section{Singular Value Decomposition}\label{chap:background:svd}

The second factorization based approach that is relevant to our work is based on the Truncated Singular Value Decomposition~\cite{golub} (TSVD). It is closely
related Principal Component Analysis~\cite{Pearson1901On} (PCA), a well established approach to dimensionality reduction.

Given a $n \times \ell$ sample matrix~\ref{def:notation:sample_matrix}
the goal is to find a best rank $k$ approximation under additional constraints:

\begin{equation}\label{eq:background:svd:opt}
\begin{aligned}
& \underset{U \in \RR^{n \times k}, S \in \RR^{k \times k}, V \in \RR^{\ell \times k}}{\text{minimize}}
& & \norm{X - U \cdot S \cdot V^T}_F, \\
& \text{subject to}
& & U^TU = I_k \\
& & & V^TV = I_k \\
& & & S = diag(\sigma), \quad \sigma \in \RR^k, \quad \sigma(i) \geq 0.
\end{aligned}
\end{equation}

The method of PCA is based on a low rank decomposition of the empirical covariance matrix, computed based on the sample matrix. The main
idea is to find a subspace that accounts for as much as variability in the data as possible. The first principal component is defined
as the one-dimensional subspace that maximizes the variance of the data when projected onto it. Formally, it solves the following problem:
\begin{equation}\label{eq:background:pca:single}
\begin{aligned}
& \underset{u \in \RR^{n}}{\text{maximize}}
& & Var(u^T \cdot X), \\
& \text{subject to}
& & \norm{u} = 1.
\end{aligned}
\end{equation}


The other principal vectors can be obtained by deflation~\cite{shawe-taylor04kernel}, or equivalently solving the eigenvalue problem:
\begin{equation}\label{eq:background:pca:opt}
\begin{aligned}
& \underset{U \in \RR^{n \times k}}{\text{minimize}}
& & \norm{Cov(X) - U \cdot \Lambda \cdot U^T}_F, \\
& \text{subject to}
& & U^TU = I_k \\
& & & \Lambda = diag(\lambda), \quad \lambda \in \RR^k.
\end{aligned}
\end{equation}

One of the main applications of PCA is as a dimensionality reduction technique, where the data is projected to the space
spanned by the normalized eigenvectors (also called principal vectors). In typical applications a truncated eigenvalue decomposition where
principal vectors with small eigenvalues are discarded (similar to truncated SVDs).

If the data matrix is centered, then the solution $U$ of TSVD and the eigenvector basis $U$ of PCA will coincide. 


\section{Canonical Correlation Analysis}\label{chap:background:cca}

Canonical Correlation Analysis (CCA) ~\cite{Hotelling} is a general procedure for studying relationships between two sets of random variables. It is based on analyzing the cross-covariance matrix between two random vectors with the aim of identifying linear relationships between them. We will start with intuitions and then give a formal presentation.

Roughly speaking, given two random vectors $\mathcal{X}$ and $\mathcal{Y}$ we are interested in "non-trivial" pairs of functions $(f,g)$ such that there is a "dependence" between $f(\mathcal{X})$ and $g(\mathcal{Y})$. The "dependence" we consider is linear (possibly in a Hilbert space). The "non-triviality" of the functions is a requirement that guards us against trivial solutions, such as $f(x) := 0 \cdot x$, $g(y) := 0 \cdot y$ - that is, $f(\mathcal{X})$ and $\mathcal{X}$ should share some information, and analogously for $g(\mathcal{Y})$ and $\mathcal{Y}$. In other words, $f$ and $g$ should not destroy the original signals. When we are interested in more than one good pair of functions, for instance, a family of pairs $(f_i,g_i)$, we typically require additional constraints to prevent non-trivial solutions by enforcing that $f_i\left(\mathcal{X}\right)$ and $f_{j \neq i}\left(\mathcal{X}\right)$ share no information, and similarly for $g_i$. We are interested in essentially different function pairs.

There are several possible applications of such an analysis. For example, a common scenario involves analyzing objects $o \in \mathcal{O}$, where $\mathcal{O}$ is some underlying space, which are not directly observable, but are only observable as images of transformations $F: \mathcal{O} \rightarrow \RR^p$ and $G: \mathcal{O} \rightarrow \RR^q$. That is, we do not have access to $o$ but only to $\left(F(o), G(o)\right)$. Then finding function pairs $(f_i, g_i)$ so that $f_i(F(o))$ behave similarly as $g_i(G(o))$ can be interpreted as finding coupled parametrizations of image spaces of $F$ and $G$ which agree on $\mathcal{O}$. This enables applications such as cross-modal information retrieval, classification, clustering, etc. If $F$ encodes a visual image and $G$ encodes a textual description of the scene, we can perform text input based search over a collection of images, see~\cite{HardoonCCA}. Bi-lingual document analysis is another application, see~\cite{mrpqr}. The pattern functions $(f_i, g_i)$ themselves can be interesting to study for exploratory purposes.

Formally, let
$$ S = \{ \left( F(o_1), G(o_1) \right), \ldots, \left( F(o_n), G(o_n) \right) \} $$
represent a sample of $n$ pairs drawn independently at random according to the underlying distribution, where $F(x_i) \in \RR^p$ and $G(x_i) \in \RR^q$ represent feature vectors from $p$ and $q$-dimensional vector spaces. Let $X=[F(o_1), \ldots, F(o_n)]$ and let $Y=[G(o_1), \ldots ,G(o_n)]$ be the matrices with observation vectors as columns (using MATLAB notation).

The idea is to find two vectors $\alpha \in \RR^p$ and $\beta \in \RR^q$ so that the random variables $\alpha^T \cdot \mathcal{X}$ and $\beta^T \cdot \mathcal{Y}$ are maximally correlated ($\alpha^T$ and $\beta^T$ map the random vectors to random variables, by computing weighted sums of vector components). By using the sample matrix notation $X$ and $Y$ this problem can be formulated as the following optimization problem:

\begin{equation}\label{eq:background:cca:original}
\begin{aligned}
& \underset{\alpha \in \RR^{p}, \beta \in \RR^{q}}{\text{maximize}}
& & \frac{\alpha^T Cov(X,Y) \beta}{\sqrt{\alpha^T Cov(X) \alpha} \sqrt{\beta^T Cov(Y) \beta}},
\end{aligned}
\end{equation}
where $Cov(X)$ and $Cov(Y)$ are empirical estimates of variances of $\mathcal{X}$ and $\mathcal{Y}$ respectively and $Cov(X,Y)$ is an estimate for the covariance matrix as defined in~\ref{def:notation:empirical_covariance} and~\ref{def:notation:empirical_cross_covariance}.
 The optimization problem can be reduced to a generalized eigenvalue problem~\cite{HardoonCCA}:
\begin{align}\label{eq:background:cca:eigen}
\begin{bmatrix}
    0       & Cov(X,Y) \\
    Cov(Y,X)& 0 
\end{bmatrix}
\cdot
\begin{bmatrix}
    \alpha \\
    \beta
\end{bmatrix}
=
\lambda
\cdot
\begin{bmatrix}
    Cov(X,X) & 0 \\
    0 &  Cov(Y,Y)
\end{bmatrix}
\cdot
\begin{bmatrix}
    \alpha \\
    \beta
\end{bmatrix}
\end{align}

If the matrices $Cov(X)$ and $Cov(Y)$ are not invertible, the problem is ill posed. One can use a regularization technique by replacing $Cov(X)$ with $(1- \kappa)Cov(X) + \kappa I$, where $\kappa \in [0,1]$ is the regularization coefficient and $I$ is the identity matrix.
A single canonical variable is usually inadequate in representing the original random vector and typically one looks for $k$ projection pairs $(\alpha_1, \beta_1),\ldots,(\alpha_k, \beta_k)$, so that $\alpha_i$ and $\beta_i$ are highly correlated and $\alpha_i$ is uncorrelated with $\alpha_j$  for $j \neq i$ and analogously for $\beta$.

The formulation in Equation~\ref{eq:background:cca:eigen} can be reformulated as a symmetric eigenvalue problem and solved efficiently. In case the dimensions of the problem $p$ and $q$ are large and observation vectors are sparse, one can consider an iterative method, for example Lanczos algorithm~\cite{LAL}). Alternatively, if the number of observation vectors $n$ is not prohibitively large, one can reformulate the problem to its dual representation which can be combined with a "kernel trick"~\cite{FBMJ} to yield nonlinear version of CCA, which will be discussed in the next section.

\section{Kernel methods}
The methods discussed so far looked for patterns expressed in the same space as the sample dataset. We now discuss how we can extend the methods to finding nonlinear patterns by using the framework of kernel methods.
To look for nonlinear patterns in the original space, one first uses a nonlinear map $\phi$ to map the input data into a Hilbert space, where linear patterns are then extracted. If the Hilbert space is high dimensional the strategy might be computationally intractable. If the feature map $\phi$ has a corresponding kernel function $\kappa$, as defined in~\ref{def:notation:kernel_function}, such that:
$$ \kappa(x,y) = \lbrace \phi(x), \phi(y) \rbrace,$$ and evaluating the kernel is feasible, then certain methods can be solved efficiently, even
if the patterns themselves lie in infinite dimensional spaces. If in a given method the data and model parameters interact only through inner products, then
we can attempt to reformulate the problem in terms of kernel matrices.

The general approach to kernelization of a method is to try to express the solution in a dual basis, that is, the basis spanned by the training instances.
The following two theorems provide an alternative characterization of kernels and relate the kernel functions with implicit feature maps. For an extended
treatment of the concepts and the proofs we refer the reader to~\cite{shawe-taylor04kernel}.

\begin{theorem}\label{thm:background:kernel_function}
A function $\kappa: X \times X \rightarrow \RR$, which is either continuous or has a finite domain
is a kernel function if and only if its kernel matrix is symmetric and positive semidefinite on any finite set of points.
\end{theorem}

\begin{theorem}\label{thm:background:representer}
Given a kernel function $\kappa: X \times X \rightarrow \RR$ we can reconstruct the implicit Hilbert space $H$ and the feature map $\phi$ as:
$$ H := \{ \sum_{i=1}^k \alpha_i \kappa(x_i, \cdot): k \in \NN, x_i \in X \},$$
$$ \phi(x) := \kappa(x, \cdot), $$ and the inner product is defined as:
$$ \langle \phi(x), \phi(y) \rangle := \kappa(x,y).$$
\end{theorem}

\subsection{Kernel $k$-means}

Instead of working directly with columns of $X$ we now work with $\phi(X(:,i)$ for a $\phi$ that corresponds to a choice of kernel $\kappa$.
Applying the iterative procedure described in~\ref{chap:background:kmeans} to the input space mapped by $\phi$ involves computing squared distances between columns $X(:,i)$ and centroids, which can be expressed as:
$$ \norm { \phi(X(:,i) - \frac{1}{|S_k|} \sum_{j \in S_k} \phi(X(:,j)) }^2,$$
where $S_k$ denotes the set of indices of points currently assigned to centroids $k$.
Since $\norm{x - y}^2 = \langle x-y, x-y \rangle = \langle x,x \rangle - 2 \langle x, y \rangle + \langle y, y \rangle$
the above quantity can be expressed using kernel evaluations as:
$$ \kappa\left(X(:,i),X(:,i)\right) - 2 \frac{1}{|S_k|} \sum_{j \in S_k} \kappa\left(X(:,i), (X(:,j) \right) + \frac{1}{|S_k|^2} \sum_{j,\ell \in S_k} \kappa\left( X(:,j), X(:,\ell) \right).$$

Using the theorem~\ref{thm:background:representer}, a new point $x$ is mapped to $\kappa(x, \cdot)$ and assigned the cluster that
minimizes
$$ argmin_k \norm{ \kappa(x,\cdot) - \frac{1}{|S_k|} \sum_{i \in S_k} \kappa\left(X(:,i), \cdot\right)}^2.$$
Again, computing the cluster assignment can be fully specified through kernel evaluations.

\subsection{Kernel PCA}

We will assume that the data is centered to simplify presentation.
The solution to PCA is expressed as an eigenvector decomposition of the covariance matrix. There is a direct correspondence between
the eigen-decompositions of the scaled covariance $(\ell - 1) Cov(X)$ and the Gram matrix $K = X^T X$.
If $(v, \lambda)$ is an eigenvector-eigenvalue pair for $K$, then $(X v, \lambda)$ is an eigenvalue pair for $(\ell - 1) Cov(X)$:
$$ (\ell - 1) Cov(X) X v = X X^T X v = X K v = \lambda X v.$$
Since $\norm{X v} = \sqrt{v^T X^T X v} = \sqrt{ \lambda v^T v } = \sqrt{\lambda}$,
the solutions to the original problem are expressed as linear combinations over the training examples of the form $\sqrt{\lambda}X v$.
Motivated by this correspondence, the kernel methods approach thus analyzes the spectrum of the kernel matrix, given a kernel function.

Since the kernel matrix is symmetric and positive-definite, the eigenvectors form an orthonormal set in the Hilbert space and
can thus be used as a projection. Analogously 
 The normalized eigenvector $v$ (with an associated $\lambda$) is expressed the Hilbert space:
$$ \sqrt{\lambda} \sum_i v(i) \phi(X(:,i))$$
which means that projecting a new point $\phi(x)$ in the kernel PCA coordinates is computed as:
$$P(\phi(x))_i := \sqrt{\lambda} \sum_i v(i) \kappa(x, X(:,i)).$$

The centering assumption is not needed, as centering can be implemented as an operation on the kernel matrix~\cite{shawe-taylor04kernel}.

\subsection{Kernel CCA}


