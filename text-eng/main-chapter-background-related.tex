%--------------------------------------------------------------------------------------------------
%
\chapter{Background}\label{chap:background}
%--------------------------------------------------------------------------------------------------

The central subjects in the thesis revolve around statistical approaches to finding structure in one, two or more sets of variates. We will
introduce two methods that find structure in a single set of variates: $k$-means clustering and Singular Value Decomposition (SVD) for dimensionality reduction, which is closely related to Principal Component Analysis (PCA). We will then present Canonical Correlation Analysis (CCA), a method for studying two sets of variates. We will also briefly cover kernel method extensions of the methods.


\section{$k$-means Decomposition}\label{chap:background:kmeans}

The $k$-means algorithm~\cite{kmeans} is perhaps the most well-known and widely-used clustering algorithm. In spirit of analysis on multiview methods that is to be presented, we will formulate $k$-means as a matrix factorization problem. Given a $n \times \ell$ training matrix~\ref{def:notation:training_matrix}
the goal is to find a best rank $k$ approximation under additional constraints:

\begin{equation}\label{eq:background:kmeans:opt}
\begin{aligned}
& \underset{C \in \RR^{n \times k}, P \in \RR^{\ell \times k}}{\text{minimize}}
& & \norm{X - C \cdot P^T}_F, \\
& \text{subject to}
& & P(i,j) \in \{0,1\}, \quad \forall i,j \\
& & & \sum_j P(i, j) = 1, \quad \forall i.
\end{aligned}
\end{equation}

The interpretation of the additional constraints on matrix $P$ is that they force each training point 
(column in $X$) to select precisely one column of $C$ to approximate it and the objective function
corresponds to minimizing a sum of squared errors made by approximating points with centroids.

In practice the optimization problem is solved by an iterative procedure~\cite{kmeans}, although it is susceptible to finding local minima. In
general, the problem is known to be NP-hard~\cite{aloise2009np}.

\section{Singular Value Decomposition}\label{chap:background:svd}

The second factorization based approach that is relevant to our work is based on the Truncated Singular Value Decomposition~\cite{golub} (TSVD). It is closely
related Principal Component Analysis~\cite{Pearson1901On} (PCA), a well established approach to dimensionality reduction.

Given a $n \times \ell$ training matrix~\ref{def:notation:training_matrix}
the goal is to find a best rank $k$ approximation under additional constraints:

\begin{equation}\label{eq:background:svd:opt}
\begin{aligned}
& \underset{U \in \RR^{n \times k}, S \in \RR^{k \times k}, V \in \RR^{\ell \times k}}{\text{minimize}}
& & \norm{X - U \cdot S \cdot V^T}_F, \\
& \text{subject to}
& & U^TU = I_k \\
& & & V^TV = I_k \\
& & & S = diag(\sigma), \quad \sigma \in \RR^k, \quad \sigma(i) \geq 0.
\end{aligned}
\end{equation}

The method of PCA is based on a low rank decomposition of the empirical covariance matrix, computed based on the training matrix. The main
idea is to find a subspace that accounts for as much as variability in the data as possible. The first principal component is defined
as the one-dimensional subspace that maximizes the variance of the data when projected onto it. Formally, it solves the following problem:
\begin{equation}\label{eq:background:pca:single}
\begin{aligned}
& \underset{u \in \RR^{n}}{\text{maximize}}
& & Var(u^T \cdot X), \\
& \text{subject to}
& & \norm{u} = 1.
\end{aligned}
\end{equation}


The other principal vectors can be obtained by deflation~\cite{shawe-taylor04kernel}, or equivalently solving the eigenvalue problem:
\begin{equation}\label{eq:background:pca:opt}
\begin{aligned}
& \underset{U \in \RR^{n \times k}}{\text{minimize}}
& & \norm{Cov(X) - U \cdot \Lambda \cdot U^T}_F, \\
& \text{subject to}
& & U^TU = I_k \\
& & & \Lambda = diag(\lambda), \quad \lambda \in \RR^k.
\end{aligned}
\end{equation}

If the data matrix is centered, then the solution $U$ of TSVD and the eigenvector basis $U$ of PCA will coincide. 



\section{Canonical Correlation Analysis}

Canonical Correlation Analysis (CCA) ~\cite{Hotelling} is a general procedure for studying relationships between two sets of random variables. It is based on analyzing the cross-covariance matrix between two random vectors with the aim of identifying linear relationships between them. We will start with intuitions and then give a formal presentation.

Roughly speaking, given two random vectors $\mathcal{X}$ and $\mathcal{Y}$ we are interested in "non-trivial" pairs of functions $(f,g)$ such that there is a "dependence" between $f(\mathcal{X})$ and $g(\mathcal{Y})$. The "dependence" we consider is linear (possibly in a Hilbert space). The "non-triviality" of the functions is a requirement that guards us against trivial solutions, such as $f(x) := 0 \cdot x$, $g(y) := 0 \cdot y$ - that is, $f(\mathcal{X})$ and $\mathcal{X}$ should share some information, and analogously for $g(\mathcal{Y})$ and $\mathcal{Y}$. In other words, $f$ and $g$ should not destroy the original signals. When we are interested in more than one good pair of functions, for instance, a family of pairs $(f_i,g_i)$, we typically require additional constraints to prevent non-trivial solutions by enforcing that $f_i\left(\mathcal{X}\right)$ and $f_{j \neq i}\left(\mathcal{X}\right)$ share no information, and similarly for $g_i$. We are interested in essentially different function pairs.

There are several possible applications of such an analysis. For example, a common scenario involves analyzing objects $o \in \mathcal{O}$, where $\mathcal{O}$ is some underlying space, which are not directly observable, but are only observable as images of transformations $F: \mathcal{O} \rightarrow \RR^p$ and $G: \mathcal{O} \rightarrow \RR^q$. That is, we do not have access to $o$ but only to $\left(F(o), G(o)\right)$. Then finding function pairs $(f_i, g_i)$ so that $f_i(F(o))$ behave similarly as $g_i(G(o))$ can be interpreted as finding coupled parametrizations of image spaces of $F$ and $G$ which agree on $\mathcal{O}$. This enables applications such as cross-modal information retrieval, classification, clustering, etc. If $F$ encodes a visual image and $G$ encodes a textual description of the scene, we can perform text input based search over a collection of images, see~\cite{HardoonSS04}. Bi-lingual document analysis is another application, see~\cite{mrpqr}. The pattern functions $(f_i, g_i)$ themselves can be interesting to study for exploratory purposes.

Formally, let
$$ S = \{ \left( F(o_1), G(o_1) \right), \ldots, \left( F(o_n), G(o_n) \right) \} $$
represent a sample of $n$ pairs drawn independently at random according to the underlying distribution, where $F(x_i) \in \RR^p$ and $G(x_i) \in \RR^q$ represent feature vectors from $p$ and $q$-dimensional vector spaces. Let $X=[F(o_1), \ldots, F(o_n)]$ and let $Y=[G(o_1), \ldots ,G(o_n)]$ be the matrices with observation vectors as columns (using MATLAB notation).

The idea is to find two linear functionals (row vectors) $\alpha \in \RR^p$ and $\beta \in \RR^q$ so that the random variables $\alpha \cdot \mathcal{X}$ and $\beta \cdot \mathcal{Y}$ are maximally correlated ($\alpha$ and $\beta$ map the random vectors to random variables, by computing weighted sums of vector components). By using the sample matrix notation $X$ and $Y$ this problem can be formulated as the following optimization problem:

\begin{equation*}
\begin{aligned}
& \underset{\alpha \in \RR^{p}, \beta \in \RR^{q}}{\text{maximize}}
& & \frac{\alpha C_{XY} \beta'}{\sqrt{\alpha C_{XX} \alpha'} \sqrt{\beta C_{YY} \beta'}},
\end{aligned}
\end{equation*}
where $C_{XX}$ and $C_{YY}$ are empirical estimates of variances of $\mathcal{X}$ and $\mathcal{Y}$ respectively and $C_{XY}$ is an estimate for the covariance matrix. Assuming that the observation vectors are centered, the matrices are computed in the following way: $C_{XX} = \frac{1}{n-1}X X'$, $C_{YY} = \frac{1}{n-1}Y Y'$ and $C_{XY} = \frac{1}{n-1}X Y'$.
The optimization problem can be reduced to an eigenvalue problem and includes inverting the variance matrices $C_{XX}$ and $C_{YY}$. If the matrices are not invertible, one can use a regularization technique by replacing $C_{XX}$ with $(1- \kappa)C_{XX} + \kappa I$, where $\kappa \in [0,1]$ is the regularization coefficient and $I$ is the identity matrix.
A single canonical variable is usually inadequate in representing the original random vector and typically one looks for $k$ projection pairs $(\alpha_1, \beta_1),\ldots,(\alpha_k, \beta_k)$, so that $\alpha_i$ and $\beta_i$ are highly correlated and $\alpha_i$ is uncorrelated with $\alpha_j$  for $j \neq i$ and analogously for $\beta$.

The problem can be reformulated as a symmetric eigenvalue problem and solved efficiently. In case the dimensions of the problem $p$ and $q$ are large and observation vectors are sparse, one can consider an iterative method, for example Lanczos algorithm~\cite{LAL}). Alternatively, if the number of observation vectors $n$ is not prohibitively large, one can reformulate the problem to its dual representation which can be combined with a "kernel trick"~\cite{FBMJ} to yield nonlinear version of CCA.

A single canonical variable is usually inadequate in representing the original random vector and typically one looks for $k$ projection pairs $(w_i^1, w_j^1),\ldots,(w_i^k, w_j^k)$, so that $(w_i^{u})^T \mathcal{X}_i$ and $(w_j^{u})^T \mathcal{X}_j$ are highly correlated and $(w_i^{u})^T \mathcal{X}_i$ is uncorrelated with $(w_i^{v})^T \mathcal{X}_i$  for $u \neq v$ and analogously for $w_j^u$ vectors.


\section{Kernels and KCCA}
