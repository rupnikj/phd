%--------------------------------------------------------------------------------------------------
%
\chapter{Background}\label{chap:background}
%--------------------------------------------------------------------------------------------------

The central subjects in the thesis revolve around statistical approaches to finding structure in one, two or more sets of variates. We will
introduce two methods that find structure in a single set of variates: $k$-means clustering and Singular Value Decomposition (SVD) for dimensionality reduction, which is closely related to Principal Component Analysis (PCA). We will then present Canonical Correlation Analysis (CCA), a method for studying two sets of variates. We will also briefly cover kernel method extensions of the methods.


\section{$k$-means Decomposition}\label{chap:background:kmeans}

The $k$-means algorithm~\cite{kmeans} is perhaps the most well-known and widely-used clustering algorithm. In spirit of analysis on multiview methods that is to be presented, we will formulate $k$-means as a matrix factorization problem. Given a $n \times \ell$ sample matrix~\ref{def:notation:sample_matrix}
the goal is to find a best rank $k$ approximation under additional constraints:

\begin{equation}\label{eq:background:kmeans:opt}
\begin{aligned}
& \underset{C \in \RR^{n \times k}, P \in \RR^{\ell \times k}}{\text{minimize}}
& & \norm{X - C \cdot P^T}_F, \\
& \text{subject to}
& & P(i,j) \in \{0,1\}, \quad \forall i,j \\
& & & \sum_j P(i, j) = 1, \quad \forall i.
\end{aligned}
\end{equation}

The interpretation of the additional constraints on matrix $P$ is that they force each sample vector 
(column in $X$) to select precisely one column of $C$ to approximate it and the objective function
corresponds to minimizing a sum of squared errors made by approximating points with centroids.

In practice the optimization problem is solved by an iterative procedure~\cite{kmeans}, although it is susceptible to finding local minima. In
general, the problem is known to be NP-hard~\cite{aloise2009np}.

\section{Singular Value Decomposition}\label{chap:background:svd}

The second factorization based approach that is relevant to our work is based on the Truncated Singular Value Decomposition~\cite{golub} (TSVD). It is closely
related Principal Component Analysis~\cite{Pearson1901On} (PCA), a well established approach to dimensionality reduction.

Given a $n \times \ell$ sample matrix~\ref{def:notation:sample_matrix}
the goal is to find a best rank $k$ approximation under additional constraints:

\begin{equation}\label{eq:background:svd:opt}
\begin{aligned}
& \underset{U \in \RR^{n \times k}, S \in \RR^{k \times k}, V \in \RR^{\ell \times k}}{\text{minimize}}
& & \norm{X - U \cdot S \cdot V^T}_F, \\
& \text{subject to}
& & U^TU = I_k \\
& & & V^TV = I_k \\
& & & S = diag(\sigma), \quad \sigma \in \RR^k, \quad \sigma(i) \geq 0.
\end{aligned}
\end{equation}

The method of PCA is based on a low rank decomposition of the empirical covariance matrix, computed based on the sample matrix. The main
idea is to find a subspace that accounts for as much as variability in the data as possible. The first principal component is defined
as the one-dimensional subspace that maximizes the variance of the data when projected onto it. Formally, it solves the following problem:
\begin{equation}\label{eq:background:pca:single}
\begin{aligned}
& \underset{u \in \RR^{n}}{\text{maximize}}
& & Var(u^T \cdot X), \\
& \text{subject to}
& & \norm{u} = 1.
\end{aligned}
\end{equation}


The other principal vectors can be obtained by deflation~\cite{shawe-taylor04kernel}, or equivalently solving the eigenvalue problem:
\begin{equation}\label{eq:background:pca:opt}
\begin{aligned}
& \underset{U \in \RR^{n \times k}}{\text{minimize}}
& & \norm{Cov(X) - U \cdot \Lambda \cdot U^T}_F, \\
& \text{subject to}
& & U^TU = I_k \\
& & & \Lambda = diag(\lambda), \quad \lambda \in \RR^k.
\end{aligned}
\end{equation}

If the data matrix is centered, then the solution $U$ of TSVD and the eigenvector basis $U$ of PCA will coincide. 


\section{Canonical Correlation Analysis}\label{chap:background:cca}

Canonical Correlation Analysis (CCA) ~\cite{Hotelling} is a general procedure for studying relationships between two sets of random variables. It is based on analyzing the cross-covariance matrix between two random vectors with the aim of identifying linear relationships between them. We will start with intuitions and then give a formal presentation.

Roughly speaking, given two random vectors $\mathcal{X}$ and $\mathcal{Y}$ we are interested in "non-trivial" pairs of functions $(f,g)$ such that there is a "dependence" between $f(\mathcal{X})$ and $g(\mathcal{Y})$. The "dependence" we consider is linear (possibly in a Hilbert space). The "non-triviality" of the functions is a requirement that guards us against trivial solutions, such as $f(x) := 0 \cdot x$, $g(y) := 0 \cdot y$ - that is, $f(\mathcal{X})$ and $\mathcal{X}$ should share some information, and analogously for $g(\mathcal{Y})$ and $\mathcal{Y}$. In other words, $f$ and $g$ should not destroy the original signals. When we are interested in more than one good pair of functions, for instance, a family of pairs $(f_i,g_i)$, we typically require additional constraints to prevent non-trivial solutions by enforcing that $f_i\left(\mathcal{X}\right)$ and $f_{j \neq i}\left(\mathcal{X}\right)$ share no information, and similarly for $g_i$. We are interested in essentially different function pairs.

There are several possible applications of such an analysis. For example, a common scenario involves analyzing objects $o \in \mathcal{O}$, where $\mathcal{O}$ is some underlying space, which are not directly observable, but are only observable as images of transformations $F: \mathcal{O} \rightarrow \RR^p$ and $G: \mathcal{O} \rightarrow \RR^q$. That is, we do not have access to $o$ but only to $\left(F(o), G(o)\right)$. Then finding function pairs $(f_i, g_i)$ so that $f_i(F(o))$ behave similarly as $g_i(G(o))$ can be interpreted as finding coupled parametrizations of image spaces of $F$ and $G$ which agree on $\mathcal{O}$. This enables applications such as cross-modal information retrieval, classification, clustering, etc. If $F$ encodes a visual image and $G$ encodes a textual description of the scene, we can perform text input based search over a collection of images, see~\cite{HardoonCCA}. Bi-lingual document analysis is another application, see~\cite{mrpqr}. The pattern functions $(f_i, g_i)$ themselves can be interesting to study for exploratory purposes.

Formally, let
$$ S = \{ \left( F(o_1), G(o_1) \right), \ldots, \left( F(o_n), G(o_n) \right) \} $$
represent a sample of $n$ pairs drawn independently at random according to the underlying distribution, where $F(x_i) \in \RR^p$ and $G(x_i) \in \RR^q$ represent feature vectors from $p$ and $q$-dimensional vector spaces. Let $X=[F(o_1), \ldots, F(o_n)]$ and let $Y=[G(o_1), \ldots ,G(o_n)]$ be the matrices with observation vectors as columns (using MATLAB notation).

The idea is to find two vectors $\alpha \in \RR^p$ and $\beta \in \RR^q$ so that the random variables $\alpha^T \cdot \mathcal{X}$ and $\beta^T \cdot \mathcal{Y}$ are maximally correlated ($\alpha^T$ and $\beta^T$ map the random vectors to random variables, by computing weighted sums of vector components). By using the sample matrix notation $X$ and $Y$ this problem can be formulated as the following optimization problem:

\begin{equation}\label{eq:background:cca:original}
\begin{aligned}
& \underset{\alpha \in \RR^{p}, \beta \in \RR^{q}}{\text{maximize}}
& & \frac{\alpha^T Cov(X,Y) \beta}{\sqrt{\alpha^T Cov(X) \alpha} \sqrt{\beta^T Cov(Y) \beta}},
\end{aligned}
\end{equation}
where $Cov(X)$ and $Cov(Y)$ are empirical estimates of variances of $\mathcal{X}$ and $\mathcal{Y}$ respectively and $Cov(X,Y)$ is an estimate for the covariance matrix as defined in~\ref{def:notation:empirical_covariance} and~\ref{def:notation:empirical_cross_covariance}.
 The optimization problem can be reduced to a generalized eigenvalue problem~\cite{HardoonCCA}:
\begin{align}\label{eq:background:cca:eigen}
\begin{bmatrix}
    0       & Cov(X,Y) \\
    Cov(Y,X)& 0 
\end{bmatrix}
\cdot
\begin{bmatrix}
    \alpha \\
    \beta
\end{bmatrix}
=
\lambda
\cdot
\begin{bmatrix}
    Cov(X,X) & 0 \\
    0 &  Cov(Y,Y)
\end{bmatrix}
\cdot
\begin{bmatrix}
    \alpha \\
    \beta
\end{bmatrix}
\end{align}

If the matrices $Cov(X)$ and $Cov(Y)$ are not invertible, the problem is ill posed. One can use a regularization technique by replacing $Cov(X)$ with $(1- \kappa)Cov(X) + \kappa I$, where $\kappa \in [0,1]$ is the regularization coefficient and $I$ is the identity matrix.
A single canonical variable is usually inadequate in representing the original random vector and typically one looks for $k$ projection pairs $(\alpha_1, \beta_1),\ldots,(\alpha_k, \beta_k)$, so that $\alpha_i$ and $\beta_i$ are highly correlated and $\alpha_i$ is uncorrelated with $\alpha_j$  for $j \neq i$ and analogously for $\beta$.

The formulation in Equation~\ref{eq:background:cca:eigen} can be reformulated as a symmetric eigenvalue problem and solved efficiently. In case the dimensions of the problem $p$ and $q$ are large and observation vectors are sparse, one can consider an iterative method, for example Lanczos algorithm~\cite{LAL}). Alternatively, if the number of observation vectors $n$ is not prohibitively large, one can reformulate the problem to its dual representation which can be combined with a "kernel trick"~\cite{FBMJ} to yield nonlinear version of CCA, which will be discussed in the next section.

\section{Kernel methods}
The methods discussed so far looked for patterns expressed in the same space as the sample dataset. We now discuss how kernel methods are applied to the methods introduced so far. 
To look for nonlinear patterns in the original space, one first uses a nonlinear map $\phi$ to map the input data into a Hilbert space, where linear patterns are then extracted. If the Hilbert space is high dimensional the strategy might be computationally intractable. If the feature map $\phi$ has a corresponding kernel function $\kappa$, as defined in~\ref{def:notation:kernel_function}, such that:
$$ \kappa(x,y) = \lbrace \phi(x), \phi(y) \rbrace,$$ and evaluating the kernel is feasible, then certain methods can be solved efficiently, even
if the patterns themselves lie in infinite dimensional spaces. If in a given method the data and model parameters interact only through inner products, then
we can attempt to reformulate the problem in terms of kernel matrices.

