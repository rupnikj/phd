%--------------------------------------------------------------------------------------------------
%
\chapter{Background}\label{chap:background}
%--------------------------------------------------------------------------------------------------

The central subjects in the thesis revolve around statistical approaches to finding structure in one, two or more sets of variates. We will
introduce two methods that find structure in a single set of variates: $k$-means clustering and Singular Value Decomposition (SVD) for dimensionality reduction, which is closely related to Principal Component Analysis (PCA). We will then present Canonical Correlation Analysis (CCA), a method for studying two sets of variates. We will also briefly cover kernel method extensions of the methods.


\section{$k$-means Decomposition}\label{chap:background:kmeans}

The $k$-means algorithm~\cite{kmeans} is perhaps the most well-known and widely-used clustering algorithm. In spirit of analysis on multiview methods that is to be presented, we will formulate $k$-means as a matrix factorization problem. Given a $n \times \ell$ sample matrix (Definition~\ref{def:notation:sample_matrix})
the goal is to find a best rank $k$ approximation under additional constraints:

\begin{equation}\label{eq:background:kmeans:opt}
\begin{aligned}
& \underset{C \in \RR^{n \times k}, P \in \RR^{\ell \times k}}{\text{minimize}}
& & \norm{X - C \cdot P^T}_F^2, \\
& \text{subject to}
& & P(i,j) \in \{0,1\}, \quad \forall i,j \\
& & & \sum_j P(i, j) = 1, \quad \forall i.
\end{aligned}
\end{equation}

The interpretation of the additional constraints on matrix $P$ is that they force each sample vector
(column in $X$) to select precisely one column of $C$ to approximate it and the objective function
corresponds to minimizing a sum of squared errors made by approximating points with centroids.

The matrix $C$ in Equation~\ref{eq:background:kmeans:opt} is uniquely defined given $P$, since for any given set
of points in $\RR^n$, the point that minimizes the sum of squared distances to the set is the mean. Since each
column of $P$ selects a subset of columns of $X$, $C$ can be expressed as:
\begin{equation}\label{eq:background:kmeans:centroids}
\begin{aligned}
 C := X\cdot P diag(\vec{1}_\ell^T \cdot P)^{-1} P^T,
\end{aligned}
\end{equation}
where the inverse of the diagonal matrix corresponds dividing by the set size when computing the mean ($\vec{1}_\ell^T \cdot P$ counts
the number of points assigned to each of the $k$ clusters). In addition, given $C$, the assignment $P$ that
minimizes the sum of squared errors can be found by:
\begin{equation}\label{eq:background:kmeans:assignments}
\begin{aligned}
P(i, j^*) = 1,\quad \textnormal{where}\quad j^* = argmin_j \norm{X(:,i) - C(:,j)}
\end{aligned}
\end{equation}

A popular approach~\cite{kmeans} to solving the problem in Equation~\ref{eq:background:kmeans:opt} is to start with an initial assignment and alternate between updating $C$ given $P$ and vice
versa. The approach is widely used in practice, even though it is susceptible to finding local minima. In general, the problem is known to be NP-hard~\cite{aloise2009np}.

\section{Singular Value Decomposition}\label{chap:background:svd}

The second factorization based approach that is relevant to our work is based on the Truncated Singular Value Decomposition~\cite{golub} (TSVD). It is closely
related to Principal Component Analysis~\cite{Pearson1901On} (PCA), a well established approach to dimensionality reduction.

Given a $n \times \ell$ sample matrix (Definition ~\ref{def:notation:sample_matrix})
the goal is to find a best rank $k$ approximation under additional constraints:

\begin{equation}\label{eq:background:svd:opt}
\begin{aligned}
& \underset{U \in \RR^{n \times k}, S \in \RR^{k \times k}, V \in \RR^{\ell \times k}}{\text{minimize}}
& & \norm{X - U \cdot S \cdot V^T}_F, \\
& \text{subject to}
& & U^TU = I_k \\
& & & V^TV = I_k \\
& & & S = diag(\sigma), \quad \sigma \in \RR^k, \quad \sigma(i) \geq 0.
\end{aligned}
\end{equation}

The method of PCA is based on a low rank decomposition of the empirical covariance matrix, computed based on the sample matrix. The main
idea is to find a subspace that accounts for as much as variability in the data as possible. The first principal component is defined
as the one-dimensional subspace that maximizes the variance of the data when projected onto it. Formally, it solves the following problem:
\begin{equation}\label{eq:background:pca:single}
\begin{aligned}
& \underset{u \in \RR^{n}}{\text{maximize}}
& & Var(u^T \cdot X), \\
& \text{subject to}
& & \norm{u} = 1.
\end{aligned}
\end{equation}


The other principal vectors can be obtained by deflation~\cite{shawe-taylor04kernel}, or equivalently solving the eigenvalue problem:
\begin{equation}\label{eq:background:pca:opt}
\begin{aligned}
& \underset{U \in \RR^{n \times k}}{\text{minimize}}
& & \norm{Cov(X) - U \cdot \Lambda \cdot U^T}_F, \\
& \text{subject to}
& & U^TU = I_k \\
& & & \Lambda = diag(\lambda), \quad \lambda \in \RR^k.
\end{aligned}
\end{equation}

One of the main applications of PCA is as a dimensionality reduction technique, where the data is projected to the space
spanned by the normalized eigenvectors (also called principal vectors). In typical applications a truncated eigenvalue decomposition where
principal vectors with small eigenvalues are discarded (similar to truncated SVDs).

If the data matrix is centered, then the solution $U$ of TSVD and the eigenvector basis $U$ of PCA will coincide.


\section{Canonical Correlation Analysis}\label{chap:background:cca}

Canonical Correlation Analysis (CCA) ~\cite{Hotelling} is a general procedure for studying
relationships between two sets of random variables. It is based on analyzing the
cross-covariance matrix between two random vectors with the aim of identifying
linear relationships between them. We will start with intuitions and then give a formal presentation.

Roughly speaking, given two random vectors $\mathcal{X}^{(1)}$ and $\mathcal{X}^{(2)}$
we are interested in ``non-trivial'' pairs of functions $(f^{(1)},f^{(2)})$ such that
there is a ``dependence'' between $f^{(1)}(\mathcal{X}^{(1)})$ and $f^{(2)}(\mathcal{X}^{(2)})$.
The ``dependence'' we consider is linear (possibly in a Hilbert space). The ``non-triviality''
of the functions is a requirement that guards us against trivial solutions, such
as $f^{(1)}(x) := 0 \cdot x$, $f^{(2)}(y) := 0 \cdot y$ - that is,
$f^{(1)}(\mathcal{X}^{(1)})$ and $\mathcal{X}^{(1)}$ should share some information,
and analogously for $f^{(2)}(\mathcal{X}^{(2)})$ and $\mathcal{X}^{(2)}$. In other words,
$f^{(1)}$ and $f^{(2)}$ should not destroy the original signals. When we are interested
in more than one good pair of functions, for instance, a family of pairs $(f_i^{(1)},f_i^{(2)})$,
we typically require additional constraints to prevent non-trivial solutions by enforcing that
$f_i^{(1)}\left(\mathcal{X}^{(1)}\right)$ and $f^{(1)}_{j \neq i}\left(\mathcal{X}^{(1)}\right)$
share no information, and similarly for $f^{(2)}_i$. We are interested in essentially
different function pairs.

There are several possible applications of such an analysis. For example, a common scenario
involves analyzing objects $o \in \mathcal{O}$, where $\mathcal{O}$ is some underlying space,
which are not directly observable, but are only observable as images of transformations
$F^{(1)}: \mathcal{O} \rightarrow \RR^p$ and $F^{(2)}: \mathcal{O} \rightarrow \RR^q$. That is,
we do not have access to $o$ but only to $\left(F^{(1)}(o), F^{(2)}(o)\right)$. Then finding
function pairs $(f^{(1)}_i, f^{(2)}_i)$ so that $f^{(1)}_i(F^{(1)}(o))$ behave similarly as $f^{(2)}_i(F^{(2)}(o))$
can be interpreted as finding coupled parametrizations of image spaces of $F^{(1)}$ and $F^{(2)}$ which
agree on $\mathcal{O}$. This enables applications such as cross-modal information retrieval,
classification, clustering, etc. If $F^{(1)}$ encodes a visual image and $F^{(2)}$ encodes a
textual description of the scene, we can perform text input based search over a collection
of images, see~\cite{HardoonCCA}. Bi-lingual document analysis is another application,
see~\cite{vinokourov2002inferring},~\cite{mrpqr}. The pattern functions $(f^{(1)}_i, f^{(2)}_i)$
themselves can be interesting to study for exploratory purposes.

Formally, let
$$ S = \{ \left( F^{(1)}(o_1), F^{(2)}(o_1) \right), \ldots, \left( F^{(1)}(o_n), F^{(2)}(o_n) \right) \} $$
represent a sample of $n$ pairs drawn independently at random according to the underlying distribution,
where $F^{(1)}(x_i) \in \RR^p$ and $F^{(2)}(x_i) \in \RR^q$ represent feature vectors from $p$ and $q$-dimensional
vector spaces. Let $X^{(1)}=[F^{(1)}(o_1), \ldots, F^{(1)}(o_n)]$ and let $X^{(2)}=[F^{(2)}(o_1), \ldots ,F^{(2)}(o_n)]$ be the matrices
with observation vectors as columns (using MATLAB notation).

The idea is to find two vectors $w^{(1)} \in \RR^p$ and $w^{(2)} \in \RR^q$ so that the random variables
$w^{(1)T} \cdot \mathcal{X}^{(1)}$ and $w^{(2)T} \cdot \mathcal{X}^{(2)}$ are maximally correlated
($w^{(1)T}$ and $w^{(2)T}$ map the random vectors to random variables, by computing weighted
sums of vector components). By using the sample matrix notation $X^{(1)}$ and $X^{(2)}$ this problem can
be formulated as the following optimization problem:

\begin{equation}\label{eq:background:cca:original}
\begin{aligned}
& \underset{w^{(1)} \in \RR^{p}, w^{(2)} \in \RR^{q}}{\text{maximize}}
& & \frac{w^{(1)T} Cov(X^{(1)},X^{(2)}) w^{(2)}}{\sqrt{w^{(1)T} Cov(X^{(1)}) w^{(1)}} \sqrt{w^{(2)T} Cov(X^{(2)}) w^{(2)}}},
\end{aligned}
\end{equation}
where $Cov(X^{(1)})$ and $Cov(X^{(2)})$ are empirical estimates of variances of $\mathcal{X}^{(1)}$ and $\mathcal{X}^{(2)}$
respectively and $Cov(X^{(1)},X^{(2)})$ is an estimate for the covariance matrix as defined
in Definition~\ref{def:notation:empirical_covariance} and Definition~\ref{def:notation:empirical_cross_covariance}.
The optimization problem can be reduced to a generalized eigenvalue problem~\cite{HardoonCCA}:
\begin{align}\label{eq:background:cca:eigen}
\begin{bmatrix}
    0       & Cov(X^{(1)},X^{(2)}) \\
    Cov(X^{(2)},X^{(1)})& 0
\end{bmatrix}
\cdot
\begin{bmatrix}
    w^{(1)} \\
    w^{(2)}
\end{bmatrix}
=\\
\lambda
\cdot
\begin{bmatrix}
    Cov(X^{(1)},X^{(1)}) & 0 \\
    0 &  Cov(X^{(2)},X^{(2)})
\end{bmatrix}
\cdot
\begin{bmatrix}
    w^{(1)} \\
    w^{(2)}.
\end{bmatrix}
\end{align}

If the matrices $Cov(X^{(1)})$ and $Cov(X^{(2)})$ are not invertible, the problem is ill posed.
One can use a regularization technique by replacing $Cov(X^{(1)})$ with $(1- \tau)Cov(X^{(1)}) + \tau I$,
where $\tau \in [0,1]$ is the regularization coefficient and $I$ is the identity matrix (and analogously for $Cov(X^{(2)})$),
see~\cite{shawe-taylor04kernel} for details.
A single canonical variable is usually inadequate in representing the original random vector and typically one
looks for $k$ projection pairs $(w^{(1)}_1, w^{(2)}_1),\ldots,(w^{(1)}_k, w^{(2)}_k)$, so that $w^{(1)}_i$ and $w^{(2)}_i$ are
highly correlated and $w^{(1)}_i$ is uncorrelated with $w^{(1)}_j$  for $j \neq i$ and analogously for $w^{(2)}$.

The formulation in Equation~\ref{eq:background:cca:eigen} can be reformulated as a symmetric eigenvalue
problem and solved efficiently. In case the dimensions of the problem $p$ and $q$ are large and
observation vectors are sparse, one can consider an iterative method, for example Lanczos
algorithm~\cite{LAL}). Alternatively, if the number of observation vectors $n$ is not prohibitively
large, one can reformulate the problem to its dual representation which can be combined with a ``kernel trick''~\cite{FBMJ}
to yield nonlinear version of CCA, which will be discussed in the next section.

\section{Kernel Methods}\label{chap:background:kernels}
The methods discussed so far looked for patterns expressed in the same space as the sample dataset. We now discuss how we can extend the methods to finding nonlinear patterns by using the framework of kernel methods.
To look for nonlinear patterns in the original space, one first uses a nonlinear map $\phi$ to map the input data into a Hilbert space, where linear patterns are then extracted. If the Hilbert space is high dimensional the strategy might be computationally intractable.
Let $\phi$ denote a feature map and $\kappa$ its kernel function as defined in Definition~\ref{def:notation:kernel_function}, that is:
$$ \kappa(x,y) = \lbrace \phi(x), \phi(y) \rbrace.$$
If evaluating the kernel is feasible, then certain methods can be solved efficiently, even
if $\phi$ is a map to an infinite dimensional space. If in a given method the data and model parameters interact only through inner products, then
we can attempt to reformulate the problem in terms of kernel matrices.

The general approach to kernelization of a method is to try to express the solution in a dual basis, that is, the basis spanned by the training instances.
The following two theorems provide an alternative characterization of kernels and relate the kernel functions with implicit feature maps. For an extended
treatment of the concepts and the proofs we refer the reader to~\cite{shawe-taylor04kernel}.

\begin{theorem}\label{thm:background:kernel_function}
A function $\kappa: X \times X \rightarrow \RR$, which is either continuous or has a finite domain
is a kernel function if and only if its kernel matrix is symmetric and positive semidefinite on any finite set of points.
\end{theorem}

\begin{theorem}\label{thm:background:representer}
Given a kernel function $\kappa: X \times X \rightarrow \RR$ we can reconstruct the implicit Hilbert space $H$ and the feature map $\phi$ as:
$$ H := \{ \sum_{i=1}^k \alpha_i \kappa(x_i, \cdot): k \in \NN, x_i \in X \},$$
$$ \phi(x) := \kappa(x, \cdot), $$ and the inner product is defined as:
$$ \langle \phi(x), \phi(y) \rangle := \kappa(x,y).$$
\end{theorem}

\subsection{Kernel $k$-means}

Instead of working directly with columns of $X$ we now work with $\phi(X(:,i)$ for a $\phi$ that corresponds to a choice of kernel $\kappa$.
Applying the iterative procedure described in Section~\ref{chap:background:kmeans} to the input space mapped by $\phi$ involves computing squared distances between columns $X(:,i)$ and centroids, which can be expressed as:
$$ \norm { \phi(X(:,i) - \frac{1}{|S_k|} \sum_{j \in S_k} \phi(X(:,j)) }^2,$$
where $S_k$ denotes the set of indices of points currently assigned to centroids $k$.
Since $\norm{x - y}^2 = \langle x-y, x-y \rangle = \langle x,x \rangle - 2 \langle x, y \rangle + \langle y, y \rangle$
the above quantity can be expressed using kernel evaluations as:
$$ \kappa\left(X(:,i),X(:,i)\right) - 2 \frac{1}{|S_k|} \sum_{j \in S_k} \kappa\left(X(:,i), (X(:,j) \right) + \frac{1}{|S_k|^2} \sum_{j,\ell \in S_k} \kappa\left( X(:,j), X(:,\ell) \right).$$

Using Theorem~\ref{thm:background:representer}, a new point $x$ is mapped to $\kappa(x, \cdot)$ and assigned the cluster that
minimizes
$$ argmin_k \norm{ \kappa(x,\cdot) - \frac{1}{|S_k|} \sum_{i \in S_k} \kappa\left(X(:,i), \cdot\right)}^2.$$
Again, computing the cluster assignment can be fully specified through kernel evaluations.

\subsection{Kernel PCA}

We will assume that the data is centered to simplify presentation.
The solution to PCA is expressed as an eigenvector decomposition of the covariance matrix. There is a direct correspondence between
the eigen-decompositions of the scaled covariance $(\ell - 1) Cov(X)$ and the Gram matrix $K = X^T X$.
If $(v, \lambda)$ is an eigenvector-eigenvalue pair for $K$, then $(X v, \lambda)$ is an eigenvalue pair for $(\ell - 1) Cov(X)$:
$$ (\ell - 1) Cov(X) X v = X X^T X v = X K v = \lambda X v.$$
Since $\norm{X v} = \sqrt{v^T X^T X v} = \sqrt{ \lambda v^T v } = \sqrt{\lambda}$,
the solutions to the original problem are expressed as linear combinations over the training examples of the form $\sqrt{\lambda}X v$.
Motivated by this correspondence, the kernel methods approach thus analyzes the spectrum of the kernel matrix, given a kernel function.

Since the kernel matrix is symmetric and positive-definite, the eigenvectors form an orthonormal set in the Hilbert space and
can thus be used as a projection. The normalized eigenvector $v$ (with an associated $\lambda$) is expressed the Hilbert space:
$$ \sqrt{\lambda} \sum_i v(i) \phi(X(:,i))$$
which means that projecting a new point $\phi(x)$ in the kernel PCA coordinates is computed as:
$$P(\phi(x))_i := \sqrt{\lambda} \sum_i v(i) \kappa(x, X(:,i)).$$

The centering assumption is not needed, as centering can be implemented as an operation on the kernel matrix, as we will
present in Section~\ref{chap:extensions:implementation}.

\subsection{Kernel CCA}

We will now present how to apply kernel methods to CCA and obtain the method known as Kernel CCA (KCCA).
The idea is to express the optimization problem in its dual form, where we express the solutions as
linear combinations of their corresponding training instances. All the interactions with the data
will be expressed through inner products, which will make the problem compatible with nonlinear feature
maps based on their respective kernels.

Let us express the vectors $w^{(1)}$ and $w^{(2)}$ in Equation~\ref{eq:background:cca:eigen} in their
dual form by using new coordinate vectors $\alpha^{(1)} \in \RR^\ell$ and $\alpha^{(2)} \in \RR^\ell$
so that:
$$w^{(1)} = X^{(1)} \alpha^{(1)},$$
$$w^{(2)} = X^{(2)} \alpha^{(2)}.$$
Assuming that the data is centered, the original optimization problem in Equation~\ref{eq:background:cca:original}
is expressed as:
\begin{equation}\label{eq:background:cca:original}
\begin{aligned}
& \underset{\alpha^{(1)} \in \RR^{\ell}, \alpha^{(2)} \in \RR^{\ell}}{\text{maximize}}
& & \frac{\alpha^{(1)T} K^{(1)} K^{(2)} \alpha^{(2)}}{\sqrt{\alpha^{(1)T} K^{(1)}K^{(1)} \alpha^{(1)}} \sqrt{\alpha^{(2)T} K^{(2)}K^{(2)} \alpha^{(2)}}}.
\end{aligned}
\end{equation}

Applying the Lagrangian multiplier technique one can arrive at the dual form of the generalized eigenvalue
problem:
\begin{align}\label{eq:background:cca:dualeigen}
\begin{bmatrix}
    0               & K^{(1)} K^{(2)} \\
    K^{(2)},K^{(1)} & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
    \alpha^{(1)} \\
    \alpha^{(2)}
\end{bmatrix}
= \\
\lambda
\cdot
\begin{bmatrix}
    K^{(1)} K^{(1)} & 0 \\
    0 &  K^{(2)} K^{(2)}
\end{bmatrix}
\cdot
\begin{bmatrix}
    \alpha^{(1)} \\
    \alpha^{(2)}.
\end{bmatrix}
\end{align}

To prevent overfitting, one can regularize the variances $(1-\tau) Cov(X^{(1)}) + \tau I_p$ and $(1-\tau) Cov(X^({2})) + \tau I_q$.
The corresponding regularized dual variances are expressed as: $(1-\tau) K^{(1)}K^{(1)} + \tau K^{(1)}$ and $(1-\tau) K^{(2)}K^{(2)}+ \tau K^{(2)}$
can then replace the diagonal blocks of the right side of Equation~\ref{eq:background:cca:dualeigen}.

